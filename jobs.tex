%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                                                          %
% Copyright (c) 2018 eBay Inc.                                             %
% Modifications copyright (c) 2019-2020 Anders Berkeman                    %
%                                                                          %
% Licensed under the Apache License, Version 2.0 (the "License");          %
% you may not use this file except in compliance with the License.         %
% You may obtain a copy of the License at                                  %
%                                                                          %
%  http://www.apache.org/licenses/LICENSE-2.0                              %
%                                                                          %
% Unless required by applicable law or agreed to in writing, software      %
% distributed under the License is distributed on an "AS IS" BASIS,        %
% WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. %
% See the License for the specific language governing permissions and      %
% limitations under the License.                                           %
%                                                                          %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%% This chapter covers most aspects of jobs and methods.  It describes
%% what methods are and how they are stored; when they are built and when
%% not; what input parameters look like in full detail; how to access
%% another job's parameters; parallel processing; return values and
%% result merging; storing and retrieving data; and how to build subjobs.




\label{chap:jobs}

\section{Definitions}

\subsection{Methods and Jobs}

In general, doing a computation on a computer follows the following
equation
\[
  \text{source code} \quad+\quad \text{input data and parameters}
  \quad+\quad \text{execution time} \quad\rightarrow\quad
  \text{result}
\]
In the Accelerator context, the notation is as follows
\[
  \text{method} \quad+\quad \text{input data} \quad+\quad \text{input
    parameters} \quad+\quad \text{execution time}
  \quad\rightarrow\quad \text{job}
\]
where the \textbf{method} is the source code, and the \textbf{job} is
a directory containing
\begin{itemize}
\item[--] any number of output files created by the running method, as well as
\item[--] a number of job meta information files containing all
  information that was needed to run the job in the first place.
\end{itemize}
The exact contents of the job directory will be discussed in
section~\ref{sec:job_directories}, but note here that the directory
contains everything needed to run a specific piece of code with inputs
and source code, \textsl{as well as} any output generated during
execution.

Computing a job is denoted job \textsl{building}.
Jobs are \textbf{built} from methods.  When a job has been built, it
is \textsl{static}, and cannot be altered or removed by the
Accelerator.  Jobs are built either by
\begin{itemize}
\item[--] a \textsl{build script}, see chapter~\ref{chap:urd}, or
\item[--] by a method, using \textsl{subjobs}, see~section~\ref{sec:subjobs}
\end{itemize}
The following figure illustrates how a job \texttt{example-1} is
built from the method \texttt{a\_method.py}.  The job is stored in the
\texttt{example} work directory.  The job identifier (in this case
\texttt{example-1}) is always unique so that it can be used as a
reference to that particular job.
\begin{figure}[h!]
  \begin{center}
    \hspace{2cm}\input{figures/method.pdftex_t}
    \label{fig:method}
  \end{center}
  \caption{When a method is built, a job directory is created in the
    target work directory, containing files with all data an meta
    information regarding the job.}
\end{figure}



\subsection{Jobids}
A \textbf{jobid} is a string that can be used as a reference to a job.
This unique string is created by appending an incrementing integer to
the name of the work directory in which the job is stored.  In the
example above, the job is uniquely identified by the
string \texttt{example-0}.



\subsection{Work Directories and Job Directories}
\label{sec:job_directories}
A successfully build of a method results in a new job directory on
disk.  The job directory will be stored in the
current \textsl{workdir} (i.e.\ work directory).  \emph{This section
is for reference only, since jobs and datasets are much more
conveniently inspected using the} \texttt{ax job} \emph{and} \texttt{ax ds}
\emph{shell commands.}

The structure of a job directory is similar to the following, assuming
the current workdir is
\texttt{test}, and the current jobid is \texttt{test-0}.
\begin{verbatim}
workdirs/test/
    test-0/
        setup.json
        method.tar.gz
        result.pickle
        post.json
        OUTPUT/
        default/
\end{verbatim}
The following table shows examples of files commonly found in a job
directory.

\starttabletwo

\texttt{setup.json} & Contains information about the job build,
including name of method, input parameters, and, after execution, some
profiling information.\\

\texttt{post.json} & Contains profiling information, and is written
only if the job builds successfully.\\

\texttt{method.tar.gz} & All source files, i.e.\ the method's source
and any \texttt{depend\_extra}s are stored in this gziped
\texttt{tar}-archive.\\

\texttt{result.pickle} & The return value from \synthesis stored in the
Python ``pickle'' format.\\

\texttt{default/} & If the job contains datasets, these will be stored
in directories, such as for example \texttt{default/}, in the root of
the job directory.\\

\texttt{datasets.txt} & List of all datasets in job in a human
readable format.\\

\texttt{OUTPUT/} & Any output to \texttt{stdout} and \texttt{stderr} will be
stored in the \texttt{OUTPUT/} directory.\\
\stoptabletwo



\subsection{The \texttt{Job} and \texttt{CurrentJob} Convenience Wrappers}
In order to simplify access to job directory data, common job data
operations are made available by the \texttt{Job} class.  There is
also an extended version of this, called the \texttt{CurrentJob}
class, that also contains information and helper functions to a
\textsl{running} job.  See section~\ref{sec:classes:job} for details about
these classes.



\section{Python Packages}
Methods are stored in standard Python packages, i.e.\ in directories
that are
\begin{itemize}
\item[--] reachable by the Python interpreter, and
\item[--] contain the (perhaps empty) file ``\texttt{\_\_init\_\_.py}''.
\end{itemize}
Packages are by default located in the same directory as the
Accelerator's configuration file, but can be changed by setting
the \texttt{package directory} configuration option in the
Accelerator's configuration file.

There are two ways, to define a package, either
using \texttt{auto-discover}, or using \texttt{methods.conf} files,
where the first is recommended for new users and the latter strongly
recommended for production systems and projects with more strict
requirements.


\subsection{A New Package using \texttt{auto-discover}}
In \texttt{auto-discover}, methods are automatically detected by the
Accelerator, and anything detected can be executed.  Create a new
directory and make it a Python package like this
\begin{shell}
% mkdir <dirname>
% touch <dirname>/__init__.py
\end{shell}
Then, add the string
\begin{shell}
<dirname> auto-discover
\end{shell}
to the \mintinline{shell}{method packages} section of the
configuration file, see section~\ref{sec:configfile}


\subsection{A New Package using \texttt{methods.conf}}
Create a new package directory
\begin{shell}
% mkdir <dirname>
% touch <dirname>/__init__.py
% touch <dirname>/methods.conf
\end{shell}
and add the string
\begin{shell}
<dirname>
\end{shell}
to the \mintinline{shell}{method packages} section of the
configuration file, see section~\ref{sec:configfile}.  The first two
lines create a Python package, and the third line adds the
file \texttt{methods.conf}, which is required in ``strict'' mode.

For security reasons, the Accelerator now only looks for packages
explicitly specified in the configuration file using the
``\mintinline{shell}{method packages}'' assignment.  Again, see
chapter~\ref{sec:configfile} for detailed information about the
configuration file.



\section{Method Source Files}
Method source files are stored in Python packages as described in the
previous section.  The Accelerator searches all reachable packages for
methods to execute, and therefore \textsl{method names need to be
globally unique}!  In order to reduce risk of executing the wrong
file, these limitations apply to methods:
\begin{enumerate}
\item the method name must be \emph{globally} unique, i.e.\ there can
  not be a method with the same name in any other method directory
  visible to the Accelerator;
\item for a method file to be accepted by the Accelerator, the
  filename has to start with the prefix ``\texttt{a\_}''; and
\item \textsl{unless} \texttt{auto-discover} is enabled (see previous section),
  the method name, \textsl{without} the ``\texttt{a\_}'' prefix, must be present on a
  separate line in the \texttt{methods.conf} file for the package, see
  section~\ref{sec:methods_conf}.
\end{enumerate}


\subsection{Creating a New Method}
In \texttt{auto-discover} mode, it is straightforward.  Any file named
\texttt{a\_name.py} will correspond to a method named \texttt{name}.
Here's how to add a method:

\begin{enumerate}
\item Create the method in a package visible to the Accelerator
  using an editor.  Make sure the filename is \texttt{a\_name.py} if
  the method's name is \texttt{name}.
\item Add the method name \texttt{name} (without the prefix ``\texttt{a\_}'' and
  suffix ``\texttt{.py}'') to the \texttt{methods.conf} file in the
  same method directory as where the source file is stored.  See
  section~\ref{sec:methods_conf}.  This is \textsl{not} needed
  if \texttt{auto-discover} is enabled, but it can still be used to
  specify a particular Python interpreter for the method, see next
  section.
\item (Remember to make sure that the method directory is defined in the Accelerator's
  configuration file.)
\end{enumerate}


\subsection{Making Methods Executable:  \texttt{methods.conf}}
\label{sec:methods_conf}
The file \texttt{methods.conf} provides an easy way to specify and
limit which methods (source files) that can be executed, and
optionally, which Python interpreter that should be used for each of
them.  Methods not specified in \texttt{methods.conf} cannot be
executed, unless \texttt{auto\_discover} is enabled.  (Exection
control is a must in production environments to prevent accidental
execution of ``forgotten'' test files etc.)

The \texttt{methods.conf} is a plain text file with one entry per
line.  Any characters from a hash sign (``\texttt{\#}'') to the end of
the line is considered to be a comment.  It is permitted to have any
number of empty lines in the file.  Available methods are entered
first on a line by stating the name of the method, without the
\texttt{a\_} prefix and \texttt{.py} suffix.

The method name can optionally be followed by one or more whitespaces
and a name specifying the actual Python interpreter that will be used
to execute the method.  This is particularly useful to have individual
methods run from individual virtual environments.  Thus, each method
can run from its own virtual environment, with its own dependencies.
A list of valid Python interpreters is defined in the configuration
file using the key ``\mintinline{shell}{interpreters}'', see
section~\ref{sec:configfile}.

The default interpreter is selected if the second field is left empty,
where default corresponds to the one that the currently running
Accelerator server is using.  The Accelerator and
its \texttt{standard\_methods} library are compatible with both Python
2 and Python 3.

Here is an example \texttt{methods.conf}
\begin{shell}
# this is a comment
test2                # will use default Python
test3           py3  # py3 as specified in accelerator.conf
testx           tf   # a Tensorflow virtual env defined in accelerator.conf
#bogusmethod    py3
\end{shell}
This file declares three methods corresponding to the files
\texttt{a\_test2.py}, \texttt{a\_test3.py}, and \texttt{a\_testx.py}.
These are the only methods that can be built in this method package.
Note that some methods are using virtual environments specified in the
Accelerator's configuration file.



\section{Job Building or Job Recycling}
Since the Accelerator keeps track of a job's dependencies and results,
it can in an instant determine if a job to be built has been built
before.  If the job has been built before, the Accelerator will
immediately return a job instance to the existing job.  Otherwise, the
job will first be built, and then a job instance will be returned.


\subsection{Job Already Built Check}
As shown in chapter~\ref{chap:urd_basic}, jobs are built using
the \texttt{.build()} function, like this
\begin{python}
def main(urd):
    urd.build('themethod', parameter1=..., ...)
\end{python}

Prior to building a method, the Accelerator checks if an equivalent
job has been built in the past.  This check is based on two things:
\begin{enumerate}
\item  the output of a hash function applied to the method source code, and
\item  the method's input parameters.
\end{enumerate}
The hash value is combined with the input parameters and compared to
all jobs already built.  Only if the hash and input parameter
combination is unique, i.e.\ it has not been seen before, will the
method be executed.  The
\texttt{.build()}-function returns an instance of type \texttt{Job}.
To the caller, it is not apparent if the job was just built or if it
was built at an earlier time.


\subsection{Depend on More Files:  \texttt{depend\_extra}}
A method may import code located in other files, and such files can be
included in the build check hash calculation as well.  This will
ensure that a change to an imported file will indeed force a
re-execution of the method if a build is requested.  Additional files
are specified in the method using the \texttt{depend\_extra} list, as
for example:
\begin{python}
from . import my_python_module

depend_extra = (my_python_module, 'mystuff.data',)
\end{python}
As seen in the example, it is possible to specify either Python module
objects or filenames relative to the method's location.

If the Accelerator suspects that a \texttt{depend\_extra}-statement is
missing, it will suggest adding it by printing a message in the output
log like this:
\begin{shelloutput}
=================================================================
WARNING: dev.a_test should probably depend_extra on myfuncs
=================================================================
\end{shelloutput}
\noindent The point of this is to make the user aware that the method
depends on additional files that are currently not taken into account
in the build check hashing.  The warning is removed by putting the
\texttt{myfuncs} file in a \texttt{depend\_extra} list of the
\texttt{test} method.


\subsection{Avoiding Rebuild: \texttt{equivalent\_hashes}}
\label{sec:equivalent_hashes}

A change to a method's source code will cause a new job to be built
upon running \mintinline{python}{.build()}, but sometimes it is
desirable to modify the source code \textsl{without} causing a
re-build.  This happens, for example, when new functionality is added
to an existing method, and re-computing all jobs is not an option.  If
functionality remains the same, existing jobs strictly do not need to
be re-built.  For this situation, there is an
\texttt{equivalent\_hashes} dictionary that can be used to \textsl{manually} specify
which versions of the source code that are equivalent.  The
Accelerator helps creating this dictionary, if needed.  This is how it
works.
\begin{enumerate}
\item Find the hash \texttt{<old\_hash>} of the existing job in that
  job's \texttt{setup.json}.
\item Add the following line to the updated method's source code
\begin{python}
equivalent_hashes = {'whatever': (<old_hash>,)}
\end{python}
\item Run the build script.  The server will print something like
\begin{shell}
===========================================================
WARNING: test_methods.a_test_rechain has equivalent_hashes,
but missing verifier <current_hash>
===========================================================
\end{shell}
\item Copy \texttt{<current\_hash>} into the
  \texttt{equivalent\_hashes}:
\begin{python}
equivalent_hashes = {<current_hash>: (<old_hash>,)}
\end{python}
\end{enumerate}
This line now tells that \texttt{current\_hash} is equivalent to
\texttt{old\_hash}, so if a job with the old hash exists, the method
will not be built again.  Note that the right part of the assignment
is actually a list, so there could be any number of equivalent
versions of the source code.  From time to time, this has been used
during development of the Accelerator's \texttt{standard\_methods},
when new features have been added that do not interfere with existing
use.


\subsection{Forcing a Rebuild}
\label{sec:force_rebuild}
It is possible to force build a job, i.e.\ build a job that already
exists.  This is mainly implemented for test code purposes, and should
be avoided in most development methodologies.  This is how to do a
force build
\begin{python}
urd.build(method, force_build=True)
\end{python}
Without forcing a re-build, all jobs in a work directory are unique.
But it is still possible to have duplicate jobs by adding/removing
workdirs and building the same job to different workdirs.  There are
no inherent problems with multiple job instances, but there is
typically no need for it, unless in testing.  Another way to force a
job to be re-built is by supplying a timestamp as input option.




\section{Invoking the Python Debugger}
Although the Accelerator is a client-server application, it is
possible to invoke the Python Debugger at any time.  The only thing
needed is to start the server with a flag like this
\begin{shell}
ax server --debuggable
\end{shell}
When this flag is set, methods will be executed in the same session as
the server with that terminal as \texttt{stdin}.  This means that it
is possible to write for example
\begin{python}
breakpoint()
\end{python}
anywhere in a method and communicate with the debugger in the server
terminal window.  Note that using this flag will kill the server if
the method fails.  See section~\ref{sec:axserver}



\section{Method Execution}
Methods are executed using the \texttt{build()} call, either from a
\texttt{build script}, or from another method as a \texttt{subjob}.
Methods typically takes input parameters, and they may generate return
values and produce output files as well as output to \texttt{stdout}
and \texttt{stderr}.


\subsection{Input Parameters}
There are three kinds of method input parameters assigned by the
\texttt{build()} call: \jobs, \datasets, and \options.  These
parameters are stated early in the method source code and are
\textsl{global}, meaning that they do not need to be included as
parameters to the functions in a method.  Here is an example parameter
set
\begin{python}
jobs = ('accumulated_costs',)
datasets = ('transaction_log', 'access_log',)
options = dict(length=4)
\end{python}
The input parameters are populated by the builder when the
\texttt{run} command is executed.  Section~\ref{sec:input_params}
and~\ref{sec:formal_options} provide detailed descriptions of all
parameters.


\subsection{Functions Reserved for Execution Flow}
During execution, methods are not run from top to bottom.  Instead,
there are three reserved functions that are called by the method
dispatcher controlling the execution flow.  These functions are
\begin{itemize}
\item [] \prepare,
\item [] \analysis, and
\item [] \synthesis.
\end{itemize}


\subsection{Execution Order}
The three functions \prepare, \analysis, and \synthesis are called one
at a time in that order.  \prepare and \synthesis execute as single
processes, while \analysis provides parallel execution.  None of them
is mandatory, but at least one must be present for the method to
execute.  It is discouraged to use \prepare only.  Use \synthesis for
single-process execution.



\subsection{Function Arguments}
There are some optional arguments that can be passed into the
executing functions \prepare, \analysis, and \synthesis at run time.
All functions have access to these
\begin{itemize}
\item[--] \texttt{job}, which is an instance of the current job, and
\item[--] \texttt{slices}, an integer holding the total number of slices.
\end{itemize}
while only \analysis has access to
\begin{itemize}
\item[--] \texttt{sliceno}, which provides a unique number to each
  parallel \analysis-process.  This is also a \textbf{mandatory}
  argument to \analysis.
\end{itemize}
The \texttt{job} instance contains information and helper functions
regarding the current job.  The object is of type \texttt{CurrentJob},
which is an extension of the \texttt{Job} class used for job instances
that are not in the execution stage.

The \analysis function (and only the \analysis function) takes the
mandatory argument \texttt{sliceno}, which is an integer between zero
and the total number of slices minus one.  This is the unique
identifier for each \analysis process, and it is commonly used when
accessing sliced datasets or doing other parallel processing tasks,
see for example chapter~\ref{chap:iterators} for its use in dataset
iterators.



\subsection{Parallel Processing:  The \analysis Function and Slices}
The number of parallel analysis processes is set by the
\texttt{slices} parameter in the Accelerator's configuration file.
The input parameter \texttt{sliceno} to the \analysis function is the
unique identifier for each parallel function call, and its value is in
the range from zero to the number of slices minus one.  When
processing Accelerator \textsl{datasets}, the idea is that each
dataset slice should have exactly one corresponding \analysis process,
so that all the slices in the dataset can be processed in parallel.


\subsection{Return Values}
Return values may be passed from one function to any function that
will execute later.  To be specific, what is returned from prepare is
called \prepareres, and can be used as input argument to \analysis and
\synthesis.  Furthermore, the return values from \analysis are
available as \analysisres in \synthesis.  The \analysisres variable is
an iterator, yielding the results from each slice in turn.  Finally,
the return value from \synthesis is stored permanently in the job
directory using the name ``\texttt{result.pickle}''.  Note
that \prepareres and \analysisres are set to \pyNone if nothing is
returned.  Here is an example of return value passing
\begin{python}
options = dict(length=4)

def prepare():
    # options is a global variable
    return options.length * 2

def analysis(sliceno, prepare_res)
    return prepare_res + sliceno

def synthesis(analysis_res, prepare_res):
     return sum(analysis_res) + prepare_res
\end{python}
Note that when a job completes, it is not possible to retrieve the
results from \prepare or \analysis anymore.  Only results from
\synthesis are kept.  Creating persistent intermediate files is the topic of
section~\ref{sec:debugflag}, however.


\subsection{Automatically Merging Results from \analysis}
It is common that results from different slices needs to be merged
together.  The Accelerator provides a relatively general function for
merging sliced data structures.  Consider this example
\begin{python}
# create a set of all users
datasets = ('source',)

def analysis(sliceno):
    return(set(datasets.source.iterate(sliceno, 'user')))

def prepare(analysis_res):
    return analysis_res.merge_auto()
\end{python}
Here, each \analysis process creates a set of \texttt{user}s seen in
that \textsl{slice} of the \texttt{source} dataset.  In order to
create a set of all \texttt{user}s in the dataset, all slice-sets have
to be merged.  Merging can be implemented using for example a
\texttt{for}-loop, but the actual merging operation is dependent of
the actual data type, and writing merging functions is error
prone. (So we've been told!)  Therefore, \analysisres has a function
called \texttt{merge\_auto()}, that is recommended for merging.  This
function can merge most data types, and even merge container variables
in a recursive fashion.  For example, a variable defined like this
\begin{python}
h = defaultdict(lambda: defaultdict(set))
\end{python}
(a \texttt{defaultdict} of \texttt{defaultdict}s of \texttt{set}s) is
straightforward to merge using \texttt{merge\_auto()}.  The function
works on many data types and is less error-prone than writing special
mergers every time they are needed. However, merging regular \texttt{dict}
objects, or objects that have an obvious substitute (e.g:
\texttt{defaultdict(int)} instead of \texttt{Counter}) might not be supported.
See the documentation in the code for the whole list of supported merges.


\subsection{Standard Out and Standard Error}
\label{sec:OUTPUT}
Anything sent to \texttt{stdout} or \texttt{stderr} during job
execution will be sent \textsl{both} to the terminal in which the
Accelerator server was started, \textsl{and} to a file in the current
job directory.  This covers, for example, anything output from
Python's \texttt{print()}-function.  Thus, the job directory contains
a complete log of all printed output from the execution phase!

Output is collected in the job directory in a subdirectory named
\texttt{OUTPUT}, and it is made available using the \texttt{.output()}
function, see section~\ref{sec:jobclass:output}.  The \texttt{OUTPUT}
directory is created \textsl{only} if anything was output from the job
to \texttt{stdout} or \texttt{stderr}, otherwise it does not exist.
Inside the directory there may be files like this
\begin{verbatim}
job-x/
   OUTPUT/
      prepare    # created if output in prepare()
      synthesis  #                      synthesis()
      0          #                      analysis() slice 0
      3          #                                       3
\end{verbatim}
No empty files will be created.


\section{The \texttt{Job} and \texttt{CurrentJob} Classes}
The \texttt{Job} and \texttt{CurrentJob} classes provide functionality
for easy access to data and datasets stored in a job directory.
(Datasets will be covered in chapter~\ref{chap:datasets}).
The \texttt{CurrentJob} is an extension of \texttt{Job} that adds
special functions that are useful to a method during execution.  This
section provides a taste of the most common operations that are
provided.  See section~\ref{sec:classes:job} for a complete list of
the functionality.

Instances of these two classes are used extensively in Accelerator
projects.  In a build script every reference to a job, such as the
return value of the
\texttt{.build()} function or any job retrieval using the Urd database
are of type \texttt{Job}.  Any job passed as input parameter to a
\texttt{.build()}-call will appear as a \texttt{Job} instance inside
the running method.  Instances of the \texttt{CurrentJob} class are
provided when asking for a \texttt{job} input parameter in
\prepare, \analysis, or \synthesis.


\subsection{Writing and Reading Serialised Data}
Data structures may be serialised and written to disk using
\texttt{job.save()} and \texttt{job.json\_save()}, with corresponding
\texttt{.load()} and \texttt{.json\_load()} functions, where the first writes a
Python ``pickle'' file, and the latter uses \texttt{JSON} encoding.
Here is an example of how to write files in a running job
\begin{python}
def synthesis(job):
   job.save('a string to be written', 'stringfile')
   job.json_save(dict(key='value'), 'jsonfile')
\end{python}
The corresponding \texttt{job.load()} and \texttt{job.json\_load()}
functions can be called \textsl{both} in methods \textsl{and} build
scripts.  For example
\begin{python}
jobs = ('anotherjob',)

def synthesis():
    jobs.anotherjob.load('stringfile')
\end{python}
will load a file from another job into the currently running method, while
\begin{python}
def main(urd):
    job = urd.build('example')
    x = job.load('thefile')
\end{python}
will load data stored by the \texttt{example} method using the
filename \texttt{thefile.pickle} into the build script.


\subsection{Writing and Reading Serialised Data in Parallel}
\label{sec:sliced_files}
If data is read and written in the parallel \analysis-function, the
argument \texttt{sliceno=} may be used to write one file for each
slice.  For example
\begin{python}
def analysis(sliceno, job)
    data = ...
    job.save(data, 'filename', sliceno=sliceno)
\end{python}
Similarly, another job can then read one of these files per slice as follows
\begin{python}

def analysis(sliceno):
    data = jobs.anotherjob.load('filename', sliceno=sliceno)
\end{python}
Writing ``sliced'' data results in $n$ files on disk, where $n$ is
equal to the number of slices set in the configuration file.  Each
filename is extended with a human readable number that corresponds to
the slice that the file's data belongs to.



\subsection{General File Access}
The \texttt{.open()} function corresponds to the built in
\texttt{open()} with the addition that it cannot write to completed
jobs, and that files written using it are book-kept in the job.  It
has to be used as a context manager, i.e.\ using the \texttt{with}
statement, for example like this
\begin{python}
def synthesis(job):
    with job.open('filename, 'wb') as fh:
        fh.write(...)
\end{python}


\subsection{Accessing a Job's Return Value}
The default behaviour of a job instance's \texttt{.load()} function is
to read the return value from the job's \synthesis function, like this
\begin{python}
def main(urd):
    job = urd.build('example')
    x = job.load()
\end{python}
This works both in build scripts and inside methods, and is a
convenient way to access data generated by a job.


\subsection{Accessing a Job's Datasets}
Using the \texttt{Job} class, it is straightforward to access datasets
in other jobs.  For example
\begin{python}
def main(urd):
    job = urd.build(...)

    # This will print a list of all dataset instances in the job.
    print(job.datasets())

    # This will return a dataset instance of the job/training dataset.
    ds = job.dataset('training')
\end{python}
This works both in running methods and in build scripts


\subsection{Accessing a Job's Options and Parameters}
\label{sec:params}
There are two sources of parameters to a running method,
\begin{itemize}
\item [] parameters from the caller, i.e.\ the \texttt{.build()}-call,
  and
\item [] parameters assigned by the Accelerator when the job starts
  building.
\end{itemize}
All these parameters are available using \texttt{job.params}.  For
example
\begin{python}
jobs = ('anotherjob',)

def synthesis():
    print(jobs.anotherjob.params.options)
\end{python}
will print the \texttt{options} dictionary that was fed to the
\texttt{anotherjob} at build time, for example
\begin{shelloutput}
{'message': 'Hello world!'}
\end{shelloutput}
\noindent A complete print of a job's \texttt{.params} may look like this
\begin{json}
{
    "starttime": 1602061144.081299,
    "endtime": 1602061147.2101562,
    "exectime": {
        "analysis": 0,
        "per_slice": [],
        "prepare": 0,
        "synthesis": 3.111,
        "total": 3.111
    },

    "caption": "",
    "hash": "9189b775e190826f3dc6ea85ea252a9e3d647185",
    "jobid": "beast-325",
    "method": "plot_walk_narrowbeams",
    "package": "dev",
    "seed": 3621427863964846452,
    "slices": 4,
    "version": 3,
    "versions": {
        "accelerator": "2020.10.3.dev1",
        "python": "3.5.2 (default, Jul 17 2020, 14:04:10) \n[GCC 5.4.0 20160609]",
        "python_path": "/home/eaenbrd/checkout/project_beast.acc/venv/bin/python3"
    },

    "options": {
        "gnbdir": 0.3839724354387525,
        "gnbpos": [
            57.71525125357654,
            12.83168
        ]
    },
    "datasets": {
        "source": "beast-222"
    },
    "jobs": {
        "background": "beast-4"
    }
}
\end{json}

\noindent and a description of its keys
\starttabletwo
\texttt{package} & Python package for this method\\
\texttt{method} & name of this method\\
\texttt{jobid} & jobid of this job\\

\texttt{starttime}& start time in epoch format\\
\texttt{endtime}& end time in epoch format\\
\texttt{exectime}& various exec times\\
\texttt{caption} & a caption\\
\texttt{slices} & number of slices of current Accelerator configuration \\
\texttt{seed} & a random seed available for use$^1$\\
\texttt{hash} & source code hash value\\
\texttt{versions} & Accelerator and Python versions\\

\texttt{options} & input parameter\\
\texttt{datasets} & input parameter\\
\texttt{jobs} &  input parameter\\
\hline\\

\multicolumn{2}{l}{\small{$^1$ The Accelerator team recommends \emph{not} using
    \texttt{seed}, unless non-determinism is actually a goal.}}\\
\stoptabletwo

Note how well covered input parameters and settings are, all the way
down to the specific Python interpreter.


\subsection{Accessing Job Output}
\label{sec:jobclass:output}
Anything written to \texttt{stderr} or \texttt{stdout} during job
execution is available using the \texttt{.output()} function.  Here is
an example
\begin{python}
def main(urd):
    job = urd.build('example')
    print(job.output())
\end{python}
With no argument, the \texttt{.output()} function returns all output.
Particular parts of the output can be selected using the options,
\texttt{'prepare'}, \texttt{'analysis'}, \texttt{'synthesis'}, or a
digit specifying a particular slice.


\subsection{Reading Post Data}
The \texttt{.post} attribute contains information such as starttime,
execution time (per function and slice), written files and subjobs for
a job.  For example
\begin{python}
def main(urd):
    job = job.build('example')
    print(job.post.exectime)
\end{python}



\section{From Jobs to Datasets and Back}
Sometimes it is necessary to find the job that created a particular
dataset, or access one of the other datasets in a job given a certain
dataset.


\subsection{From Dataset to Job}
\begin{python}
  job = ds.job
\end{python}


\subsection{From Job to Dataset}
\begin{python}
  ds = job.dataset("datasetname")
\end{python}


\subsection{From Dataset to Dataset (in same Job)}
\begin{python}
  ds = ds.job.dataset("datasetname")
\end{python}




\section{Method Input Parameters}
\label{sec:input_params}

There are three kinds of method input parameters assign by
the \texttt{build} call: \jobs, \datasets, and \options.  These
parameters are stated early in the method source code, such as for
example
\begin{python}
jobs = ('accumulated_costs',)
datasets = ('transaction_log', 'access_log',)
options = dict(length=4)
\end{python}
The input parameters are populated by the builder using name matching
when the job build is initiated, see chapter~\ref{chap:urd} for more
information.

The \texttt{jobs} parameter list is used to input references to other
jobs, while the \datasets parameter list is used to input references
to datasets.  These parameters are populated by the build call.

The \options dictionary, on the other hand, is used to input any other
type of parameters to be used by the method at run time.  Options does
not necessarily have to be populated by the build call.  Instead,
``default values'' may be used as ``global constants'' in the method.
An option assigned by the build call will override the default
assignment.

Note that \jobs and \datasets are \texttt{tuple}s (or \texttt{list}s
or \texttt{set}s), and a single entry has to be followed by a comma as
in the example above, while \options is a dictionary.  Individual
elements of the input parameters may be accessed inside the method
using dot notation like this
\begin{python}
jobs.accumulated_cost
datasets.transaction_log
options.length
\end{python}
Each of these parameters will be described in more detail in following
sections.


\subsection{Input Jobs}
The \jobs parameter is a tuple of job references linking other jobs to
this job.  In a running job, each item in the \jobs tuple is of
type \texttt{Job}, and it is possible to used them directly as
references to corresponding jobs.  All items in the \jobs tuple must
be assigned by the builder to avoid run time errors.

If the number of input jobs is not constant or known beforehand, they
can be represented as a list, like in this example
\begin{python}
jobs = ('source', ['alistofjobs'],)
\end{python}
where \texttt{source} is a single job reference, whereas
\texttt{alistofjobs} is a list of job references.


\subsection{Input Datasets}
The \datasets parameter is a tuple of links to datasets.  In a running
job, each item in the \datasets variable is of type
\texttt{Dataset}.  The \texttt{Dataset} class is further described in
chapter~\ref{chap:datasets}.  All items in the \datasets tuple must be
assigned by the builder to avoid run time errors.

Similar to \jobs, one can represent an unknown number datasets using a
list, like this
\begin{python}
datasets = ('source', ['alistofdatasets'],)
\end{python}
where \texttt{source} is a single dataset, whereas
\texttt{alistofdatasets} is a list of datasets.



\subsection{Input Options}

The \options parameter is of type \texttt{dict} and is used to pass
various information from the builder to a job.  This information could
be integers, strings, enumerations, sets, lists, and dictionaries in a
recursive fashion, with or without default values.  Assigning defined
options from a build call is not necessary, and an assignment will
override the ``default'' specified, if any.  Options are specified
like in this example
\begin{python}
  options = dict(key=value, ... )  # or
  options = {key: value, ...}
\end{python}

Options are straightforward to use and quite flexible.  A formal
overview is presented in section~\ref{sec:formal_options}.



\subsection{A Specific File From Another Job:  \texttt{JobWithFile}}
\index{input options!\texttt{JobWithFile}}
\label{sec:jobwithfile}
Any specific file from an existing job can be input to a new job at
build time using \texttt{job.withfile()}.  Here is an example
\begin{python}
def main(urd):
    job = urd.build('example4')
    urd.build(
        'example5',
        firstfile=job.withfile('myfile1', sliced=True),
        secondfile=job.withfile('myfile2'),
    )
\end{python}
Inside the method, the \texttt{option} part is defined like this
\begin{python}
from accelerator import JobWithFile
options=dict(firstfile=JobWithFile, secondfile=JobWithFile)
\end{python}
The \texttt{.withfile()} function requires a filename and takes two
optional arguments: \texttt{sliced} and \texttt{extras}.  The
\texttt{extras} argument is used to pass any kind of information that
is helpful when using the specified file, and \texttt{sliced} tells
that the file is stored in parallel slices.  (Creating sliced files is
described in section~\ref{sec:sliced_files}.)

Here's how to use the \texttt{JobWithFile} object in a running job.
First, loading a file is done using \texttt{.load()}, like this
\begin{python}
from accelerator import JobWithFile
options=dict(firstfile=JobWithFile, secondfile=JobWithFile)

def analysis(sliceno):
    print(options.firstfile.load(sliceno=sliceno))

def synthesis():
    print(options.secondfile.load())
\end{python}
The \texttt{.load()} function assumes that the file is in Python
Pickle format.  There is also an \texttt{.json\_load()} function
for \texttt{JSON}-files.  To get the full filename to the file,
use \texttt{.filename()}
\begin{python}
    print(options.firstfile.filename(sliceno=3))
    print(options.secondfile.filename())
\end{python}
And finally, there is also a wrapper around \texttt{open()}, so it is
possible to do
\begin{python}
    with(options.firstfile.open(), 'rb') as fh:
        data = fh.read()
\end{python}


\section{Subjobs}
\index{subjobs}
\label{sec:subjobs}
Jobs may launch jobs, i.e.\ a running job may build other jobs in a
recursive manner.  As always, if the jobs have been built already,
they will be linked in immediately.  If the build of a subjob fails,
the building job will be invalidated.  Subjobs are built like in this
example
\begin{python}
from accelerator import subjobs

def prepare():
    subjobs.build('count_items', options=dict(length=3))
\end{python}
It is possible to build subjobs in \prepare and \synthesis, but \textsl{not} in
\analysis.  The \texttt{subjobs.build()} call uses the same syntax as
\texttt{urd.build()} described in chapter~\ref{chap:urd}, so input
parameters like \options, \datasets, \jobs, and \texttt{caption} are
available for subjobs too.  Similarly, the return value from a subjob
\texttt{build()} is a job instance corresponding to the built job.

There are three catches, though.
\begin{enumerate}
\item
Dataset instances to datasets created in subjobs will not be
explicitly available to the ``top level'' build script.  The workaround
is to link the dataset to the building method like this
\index{\texttt{link\_to\_here}}
\begin{python}
from accelerator import subjobs
def synthesis():
    job = subjobs.build('create_a_dataset')
    ds = job.dataset(<name>)
    ds.link_to_here(name=<anothername>)
\end{python}
with the effect that the building job will act like a dataset, even
though the dataset is actually created and stored in the subjob.  The
\texttt{name} argument is optional, the name \texttt{default} is used
if left empty, since this is the default dataset name.

It is also possible to override the dataset's \textsl{previous
dataset} using the \texttt{override\_previous} option, which takes a
dataset reference (or \pyNone) to be the
new \texttt{previous}.  \index{\texttt{override\_previous}}
\begin{python}
    ds.link_to_here(name='thename', override_previous=xxx)
\end{python}
The \texttt{link\_to\_here} call returns the new dataset instance.

\item
  Subjobs are not visible in build scripts, and do not show up in
  for example \texttt{urd.joblist}.

\end{enumerate}
There is a limit to the recursion depth of subjobs, to avoid creating
unlimited number of jobs by accident.  The limit can be tweaked by
modifying the source code, if necessary.



\section{Formal Option Rules}
\label{sec:formal_options}
This section covers the formal rules for the \options parameter.
\begin{enumerate}

\item Typing may be specified using the class name
  (e.g.\ \py{int}), or as a value of the desired type
  (e.g.\ the number \py{3}).  See this example
  \begin{python}
options = dict(
    a=3,     # typed to int
    b=int,   #          int
    c=3.14,  #          float
    d='',    #          str
)
  \end{python}
  Values will be default values, and this is described thoroughly in
  the other rules.

 \item An input option value is required to be of the correct type.
   This is, if a type is specified for an option, this must be
   respected by the builder.  Regardless of type,
   \mintinline{python}/None/ is always accepted.

\item An input may be left unassigned, unless
  \begin{itemize}
  \item the option is typed to \mintinline{python}/RequiredOption()/, or
  \item the option is typed to \mintinline{python}/OptionEnum()/ without a default.
  \end{itemize}
  So, except for the two cases above, it is not necessary to supply
  option values to a method at build time.

\item If typing is specified as a value, this is the default value if
  left unspecified.

\item If typing is specified as a class name, default is
  \mintinline{python}/None/.

\item \mintinline{python}/None/ is always a valid input unless
  \begin{itemize}
  \item \py{RequiredOption()} without \py{none_ok=True}
    \index{input options!\texttt{RequiredOption}}
  \item \py{OptionEnum()} without \py{none_ok=True}
    \index{input options!\texttt{OptionEnum}}
  \end{itemize}
  This means that for example something typed to \py{int} can be
  overridden by the builder by assigning it to
  \mintinline{python}/None/.  Also, \mintinline{python}/None/ is also
  accepted in typed containers, so a type defined as
  \mintinline{python}/[int]/ will accept the input
  \mintinline{python}/[1, 2, None]/.

\item All containers can be specified as empty, for example
  \py{{}} which expects a \py{dict}.

\item Complex types (like \texttt{dict}s, \texttt{dict}s of
  \texttt{list}s of \texttt{dict}s, \dots) never enforce specific
  keys, only types.  For example, \mintinline{python}/{'a': 'b'}/
  defines a dictionary from strings to strings, and for example
  \mintinline{python}/{'foo': 'bar'}/ is a valid
  assignment.

\item Containers with a type in the values default to \pyNone.
  Otherwise the specified values are the default contents.  Example
  \begin{python}
options = dict(
    x=dict,        # will be None as default, accepts any dict
    y={'a': 'b'},  # will be {'a': 'b'} as default, accepts dict of str
    z={str: str},  # will be {} as default, accepts dict of str
)
  \end{python}
\end{enumerate}

The following sections will describe typing in more detail.



\subsection{Options with no Type}
An option with no typing may be specified by assigning \texttt{None}.
\begin{python}
options = dict(length=None)  # accepts anything, default is None
\end{python}
Here, \texttt{length} could be set to anything.



\subsection{Scalar Options}
Scalars are either explicitly typed, as
\begin{python}
options = dict(length=int)   # Requires an int value or None
\end{python}
or implicitly with default value like
\begin{python}
options = dict(length=3)     # Requires an int value or None,
                             # default is 3 if left unassigned
\end{python}



\subsection{String Options}
A (possibly empty) string with default value \mintinline{python}{None} is typed as
\begin{python}
options = dict(name=str)     # requires string or None, defaults to None
\end{python}
A default value may be specified as follows
\begin{python}
options = dict(name='foo')   # requires string or None, provides default value
\end{python}
And a string required to be specified and none-empty as
\index{input options!\texttt{OptionString}}
\begin{python}
from accelerator import OptionString
options = dict(name=OptionString)       # requires non-empty string
\end{python}
In some situations, an example string is convenient
\begin{python}
from accelerator import OptionString
options = dict(name=OptionString('bar') # Requires non-empty string,
                                        # provides example (NOT default value)
\end{python}
Note that ``\texttt{bar}'' is not default, it just gives the
programmer a way to express what is expected.



\subsection{Enumerated Options}
Enumerations are convenient in a number of situations.  An option with
three enumerations is typed as
\begin{python}
# Requires one of the strings 'a', 'b' or 'c'
from accelerator import OptionEnum
options = dict(foo=OptionEnum('a b c'))
\end{python}
and there is a flag to have it accept \mintinline{python}/None/ too
\begin{python}
# Requires one of the strings 'a', 'b', or 'c'; or None
from accelerator import OptionEnum
options = dict(foo=OptionEnum('a b c', none_ok=True))
\end{python}
A default value may be specified like this
\begin{python}
# Requires one of the strings 'a', 'b' or 'c', defaults to 'b'
from accelerator import OptionEnum
options = dict(foo=OptionEnum('a b c').b)
\end{python}
(The \texttt{none\_ok} flag may be combined with a default value.)
Furthermore, the asterisk-wildcard could be used to accept a wide
range of strings
\begin{python}
# Requires one of the strings 'a', 'b', or any string starting with 'c'
options = dict(foo=OptionEnum('a b c*'))
\end{python}
The example above allows the strings ``\texttt{a}'', ``\texttt{b}'',
and all strings starting with the character ``\texttt{c}''.



\subsection{List and Set Options}
Lists are specified like this
\begin{python}
# Requires list of intable or None, defaults to empty list
options=dict(foo=[int])
\end{python}
Empty lists are accepted, as well as \mintinline{python}/None/.  In
addition, \mintinline{python}/None/ is also valid inside the list.
Sets are defined similarly
\begin{python}
# Requires set of intable or None, defaults to empty set
options=dict(foo={int})
\end{python}
Here too, both \mintinline{python}/None/ or the empty \texttt{set} is
accepted, and \mintinline{python}/None/ is a valid set member.



\subsection{Date and Time Options}
The following date and time related types are supported:
\begin{itemize}
\item[] \texttt{datetime},
\item[] \texttt{date},
\item[] \texttt{time}, and
\item[] \texttt{timedelta}.
\end{itemize}
A typical use case is as follows
\begin{python}
# a datetime object if input, or None
from datetime import datetime
options = dict(ts=datetime)
\end{python}
and with a default assignment
\begin{python}
#  a datetime object if input, defaults to a datetime(2014, 1, 1) object
from datetime import datetime
options = dict(ts=datetime(2014, 1, 1))
\end{python}



\subsection{More Complex Stuff:  Types Containing Types}
It is possible to have more complex types, such as dictionaries of
dictionaries and so on, for example
\begin{python}
# Requires dict of string to string
options = dict(foo={str: str})
\end{python}
or another example
\begin{python}
# Requires dict of string to dict of string to int
options = dict(foo={str: {str: int}})
\end{python}
As always, containers with a type in the values default to empty
containers.  Otherwise, the specified values are the default contents.






\section{Jobs - a Summary}
The concepts relating to Accelerator jobs are fundamental, and this
section provides a shorter summary about the basic concepts.

\begin{itemize}
\item[1.]  Data and metadata relating to a job is stored in a
job directory.
\item[2.]  Job objects are wrappers around job directories, providing helper functions.
\end{itemize}
The files stored in the job directory at dispatch are complete in the
sense that they contain all information required to run the job.  So
the Accelerator job dispatcher actually just creates processes and
points them to the job directory.  New processes have to go and figure
out their purpose by themselves by looking in this directory.

A running job has the process' \textsl{current working directory
(CWD)} pointing into the job directory, so any files created by the
job (including return values) will by default be stored in the job's
directory.

When a job completes, the meta data files are updated with profiling
information, such as execution time spent in single and parallel
processing modes.

All code that is directly related to the job is also stored in the job
directory in a compressed archive.  This archive is typically limited
to the method's source, but the code may have manually added
dependencies to any other files, and in that case these will be added
too.  This way, source code and results are always connected and
conveniently stored in the same directory for future reference.
\begin{itemize}
\item[3.]  Unique jobs are only executed once.
\end{itemize}
Among the meta information stored in the job directory is a hash
digest of the method's source code (including manually added
dependencies).  This hash, together with the input parameters, is used
to figure out if a result could be re-used instead of re-computed.
This brings a number of attractive advantages.
\begin{itemize}
\item[4.]  Jobs may link to each other using job references.
\end{itemize}
Which means that jobs may share results and parameters with each other.
\begin{itemize}
\item[5.]  Jobs are stored in workdirs.
\item[6.]  There may be any number of workdirs.
\end{itemize}
This adds a layer of ``physical separation''.  All jobs relating to
importing a set of data may be stored in one workdir, perhaps named
\texttt{import}, and development work may be stored in a workdir
\texttt{dev}, etc.
\begin{itemize}
\item[7.] Jobids are created by appending a counter to the
workdir name, so a job \texttt{dev-42} may access data in
\texttt{import-37}, and so on, which helps manual inspection.
\end{itemize}
\begin{itemize}
\item[8.] Jobs may dispatch other jobs.
\end{itemize}
It is perfectly fine for a job to dispatch any number of new jobs, and
these jobs are called \textsl{subjobs}.  A maximum allowed recursion
depth is defined to avoid infinite recursion.
