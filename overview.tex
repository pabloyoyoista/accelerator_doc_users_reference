%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                                                          %
% Copyright (c) 2018 eBay Inc.                                             %
% Modifications copyright (c) 2019-2021 Anders Berkeman                    %
%                                                                          %
% Licensed under the Apache License, Version 2.0 (the "License");          %
% you may not use this file except in compliance with the License.         %
% You may obtain a copy of the License at                                  %
%                                                                          %
%  http://www.apache.org/licenses/LICENSE-2.0                              %
%                                                                          %
% Unless required by applicable law or agreed to in writing, software      %
% distributed under the License is distributed on an "AS IS" BASIS,        %
% WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. %
% See the License for the specific language governing permissions and      %
% limitations under the License.                                           %
%                                                                          %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%





\section{High Level View}
The Accelerator is a client-server based application, and from a high
level, it can be visualised like in figure~\ref{fig:overview}.

\begin{figure}[h!]
  \begin{center}
    \vspace{2ex}
    \input{figures/overview.pdftex_t}
    \caption{High level view of the Accelerator framework.  See text
      for details.}
    \label{fig:overview}
  \end{center}
\end{figure}

The left part of the figure shows the \emph{client} side, with shell
commands and web interface.  On the right side is the \emph{server}
and its databases.  The Accelerator executes \textsl{build scripts},
that start \textsl{jobs} on the server.  These jobs are stored in the
server's job databases for later retreival.  There are two databases,
one for storing everything related to a job's execution, called the
\textsl{workdir}, and one transaction log used for storing
meta-information about built jobs.  The workdir will contain all
inputs, source code, parameters, and outputs of all executed jobs,
whereas the job log database ensures reproducibility and transparency,
and it will be further discussed in chapter~\ref{chap:urd}.


\section{Interacting with the Accelerator}
There are two ways to interface the Accelerator, using
\begin{itemize}
\item[] the command line, and
\item[] using a web brower.
\end{itemize}
The command line interface is based on the \texttt{ax} shell command.
It can be used for a large variety of operations, including starting
execution of scripts, listing or grepping data in datasets, finding
particular jobs, and a lot more.

The web interface, called \textsl{Accelerator Board}, is used to
inspect jobs, datasets, and meta-information.  Resulting files such as
images, text files, pdfs, video files and so on are rendered in the
web browser.


\section{Jobs}
A problem that is solved using the Accelerator is typically
partitioned into smaller programs called \emph{methods}.  A method is
a Python program with one or a few special functions used to execute
code sequentially or in parallel, and to pass parameters and results.

For each method that is executed by the Accelerator, a unique new
directory, called \emph{job directory}, is created in which everything
related to the execution, such as inputs, outputs, source code etc.,
is stored.  An executed method is called a \emph{job}.  To be truly
transparent, the job directory will also contain profiling
information, Python interpreter version and other related information.

The Accelerator has a database that keeps track of all jobs that has
been run.  This avoids unnecessary re-computing in favour of reusing
previously computed results.  Reusing jobs does not only speed up
processing and encourage incremental design, but also makes it
transparent which code and which data that was used for any particular
result, minimising these kind of uncertainties to zero.


\subsection{A ``Hello, World'' Example}
Here's an example of a very simple method.  It does not take any input
parameters and does almost nothing, it will just return the string
``\texttt{hello world}'' and exit.  The example is stored in a file
named \texttt{a\_hello\_world.py}\footnote{There's a reason for the
file name prefix, see section~\ref{@@@@@@@@@@@@}.}:
\begin{python}
def synthesis():
    return "hello world"
\end{python}
The \texttt{synthesis()}-function is reserved for code that is to be
executed once without parallellisation.  The method is stored in a
separate file.  In order to get the method to execute, it is called
from a \textsl{build script} looking something like this
\begin{python}
def main(urd):
    job = urd.build('hello_world')
    print(job.load())
\end{python}
and stored in a file \texttt{build\_mytest.py}.  The
\texttt{main()}-function is called when the build script is run.  The
input parameter \texttt{urd} is an object that contains functions for
job building and organisation, and is described in
chapter~\ref{chap:urd} and section~\ref{sec:classes:urd}.  The build
script is executed by typing a shell command like this
\begin{shell}
ax run mytest
\end{shell}
In the build script, the \texttt{urd.build()}-call will instruct the
server to execute the \texttt{hello\_world} method.  During
its execution, a job directory will be created that contains
everything associated with the build process.  When the
\texttt{build()}-call is completed, a job object, of type
\texttt{Job}, is returned to the program.  This object provides a
convenient interface to the data in the corresponding job directory,
and contains member functions such as \texttt{.load()}, that is used
in the example to read back the returned value from the job.

Note that what is returned by the method is stored in a file on disk.
There is no need to know the name of the file, since its contents can
be retrieved by the job objects \texttt{load()} function.



\subsection{Jobs Can Only be Run Once}
If the build script is executed again, the \texttt{hello\_world} job
will \emph{not} be re-built, because the Accelerator remembers that
the job has been built in the past, and all associated information is
already stored in a job directory.  Instead, the Accelerator
immediately returns a job object representing the previous run.  This
means that from a user's perspective, there is no visible difference
between running a job for the first time or re-using results from an
existing run!  In order to have the method execute again, either its
source code or input parameters has to change.  If there are such
changes, the method will be re-executed, and a new job will be created
that reflects these changes.


\begin{figure}[b!]
  \hspace{1.5cm}\input{figures/job0.pdftex_t}
    \caption{A simple hello world program, represented as graph and
      work directory.  Building the \texttt{hello\_world} method
      creates a \emph{job} \texttt{test-0}.  This job is stored in a
      (job) directory, using the same name, in the \texttt{test}
      \emph{workdir}.  Everything associated with the build is stored
      in this job directory.}
    \label{fig:execflow-hello-world}
\end{figure}


\clearpage


\subsection{A Quick Look Under the Hood}
The Accelerator is fully transparent, and no data is ``hidden''
anywere.  There is no need to know how or where things are stored to
use the Accelerator, but still it could be of interest to know where
things ends up (and that it is straightforward to access things using
simple shell commands.)

Figure~\ref{fig:execflow-hello-world} illustrates an execution of the
\texttt{hello\_world} method.  The created job gets the \textsl{jobid}
\texttt{test-0}, and parts of the corresponding job directory
information is shown in green.  Jobids are job identifiers, named by
their corresponding \textsl{workdir} plus an integer counter value.


The most important files in a job directory are
\begin{itemize}
\item[] \texttt{setup.json}, containing job meta information;
\item[] \texttt{result.pickle}, containing the returned data; and
\item[] \texttt{method.tar.gz}, containing the method's source code,
\end{itemize}
and in general the job directory could contain any number of files.

The \texttt{Job} object, which is returned by the build call, provides
a convenient way to access files and data stored in the job directory.
For example, the job's return value can be loaded into a variable
using the \texttt{.load()} function, all metadata can be pretty
printed using the \texttt{ax job} shell command, and so on.


\subsection{Workdirs and Sharing Jobs}

A workdir is where jobs are stored.  There can be more than one
workdir, and different workdirs can be used separate jobs into
different physical locations, or to separate different types of data
from one another in the same project.  The Accelerator can be set up
to have any number of workdirs accessible, but typically only one is
used for writing.

Entering the same workdir definition into two or more users'
configuration files, the workdir and its contents will be shared
between the users.  Each Accelerator server will update its knowledge
about the contents of all workdirs before executing a build script, to
make sure that the latest jobs are taken into account.  In addition,
the job log database, as described in chapter~\ref{chap:urd}, is
designed for efficient re-use and sharing of particularly interesting
jobs.  Thus, only one user has to execute a given method for all other
users to have instant access to it.  This can save a lot of time in a
project.


\begin{figure}[b]
    \hspace{1.5cm}\input{figures/job0job1.pdftex_t}
    \caption{Job \texttt{test-0}, is used as input to the
      \texttt{test-1} job.  Jobs are created from methods
      (i.e.\ source code), input parameters, and processing time using
      the \texttt{build()} call.  Each job corresponds to a number of
      files stored in a per job unique (job) directory.}
    \label{fig:execflow-print-result}
\end{figure}




\subsection{Linking Jobs}

Using jobs, complex tasks can be split into several smaller
operations.  Jobs can be connected to eachother so that a new job will
use results from one or more previously executed jobs.

To continue the simple hello world example, assume for a second that
the \texttt{hello\_world}-job is computationally expensive, and should
therefore be run as few times as possible to save time and energy.
The job creates a result that will be used in further processing.
This further processing is represented in this example by printing the
result to standard output.

\clearpage

\noindent A new method \texttt{print\_result} is
created for this task, and it looks like this
\begin{python}
jobs = ('hello_world_job',)

def synthesis():
    data = jobs.hello_world_job.load() 
    print(data)
\end{python}
The ``\texttt{jobs=}''-line at the top specifies that the method
expects an input of job type to be provided at run time.  The input is
arbitrarily named \texttt{hello\_world\_job}, and the variable
\texttt{jobs.hello\_world\_job} will be of job-type, and therefore its
result can be read using the \texttt{.load()} function as shown
previously.

The following extended build script will run both the first and second
method, and provide the first job as input to the second:
\begin{python}
def main(urd):
    job1 = urd.build('hello_world')
    job2 = urd.build('print_result', hello_world_job=job1)
\end{python}

Figure~\ref{fig:execflow-print-result} illustrates the situation.
Note the direction of the arrow.  Arrows illustrate pointers to
previous jobs.  In this example, the second job, \texttt{test-1}, has
\texttt{test-0} as input parameter, i.e.\ \texttt{test-1} has a
pointer to \texttt{test-0}.  (And \texttt{test-0} does not know of any
jobs run in the future, so the arrow cannot point out from it.)

The example shows how a complex task may be split into several jobs,
each reading intermediate results from previous ones, \emph{without
the need for explicit filenames}!  The Accelerator will keep track of
all job dependencies, so there is no doubt which jobs that are run
when, and on which data.  And since the Accelerator remembers if a job
has been executed before, it will link and \emph{re-use previous jobs
when possible}, thereby saving time and energy.

Also note that a re-used job is a \emph{proof} of that the code,
input- and output data is unchanged and connected.  \emph{Using the
Accelerator, there is no risk that wrong intermediate files are used
by mistake}, since the files are accessed by reference to the method
that created them, and noy by an arbitrary user-selected filename!


\subsection{Job Input Parameters}
In the previous section it is shown how the input parameter \jobs is
used to point to earlier jobs.  There are two more types of input
parameters: \options and \datasets, and here's an example of how to
declare all of them in a method:
\begin{python}
jobs = ('hello_world_job',)   # as seen in the previous example
datasets = ('transaction_log', 'access_log',)
options = dict(length=4)
\end{python}
All paramteres are populated by the \texttt{build()} call in the build
script.  The \jobs parameter is used to link the job to previous jobs.
\options is a dict of any kind of options the job may use, and
\datasets is, similar to \jobs, a list of datasets.  (Datasets will be
introduced shortly.)


\subsection{Parallel Processing}

While the \texttt{synthesis()}-function (presented in the previous
section) is used for code that is to execute exactly once, the
\texttt{analysis()}-function is used for simple but powerful parallel
processing.  A method making use of parallel processin may look like
this
\begin{python}
def analysis(sliceno):
    print('This is process %d!' % (sliceno,))
\end{python}
When the method is executed, the \texttt{analysis()}-function will be
forked into \texttt{sliceno} parallel processes (where
\texttt{sliceno} is specified in the Accelerator's configuration file
and typically set to the number of available CPU cores of the
machine).  The input parameter \texttt{sliceno} is a number between
zero and \texttt{sliceno-1} and used to identify each process.

Admittedly, this is the most basic form of parallel processing, that
can only solve a subset of all kinds of parallel problems.
Nevertheless, it solves common data processing problems, and it solves
them a very simple and efficient manner.  The next section shows how
the Accelerator makes use of this to do data processing on all CPUs in
parallel.


\subsection{Job Execution Flow}

Execution of code in a method is either parallel or serial depending
on which function is used to encapsulate it.  In addition to
\texttt{analysis()} and \texttt{synthesis()} that have already been
introduced, there is a third one named \texttt{prepare()}.  All three
may exist in the same method, and at least one is required.  When the
method executes, they are called one after the other.

Results can be transferred from one function to the next using return
values.  There is also an easy way to merge results from parallel
processing (introduced in section~\ref{jobs}).

\begin{itemize}
\item[] \texttt{prepare()} is executed first.  The returned value is
  available in the variable \texttt{prepare\_res}.
\item[] \texttt{analysis()} is run in parallel processes, one for each
  slice.  It is called after completion of \texttt{prepare()}.  Common
  input parameters are \texttt{sliceno}, holding the number of the
  current process instance, and \texttt{prepare\_res}.  The return
  value for \emph{all} processes becomes available in the
  \texttt{analysis\_res} generator variable.
\item[] \texttt{synthesis()} is called after the last
  \texttt{analysis()}-process is completed.  It is typically used to
  aggregate parallel results created by \texttt{analysis()} and takes
  both \texttt{prepare\_res} and \texttt{analysis\_res} as optional
  input parameters.
\end{itemize}
The return value from \texttt{synthesis()} is the default return value
of the job itself.  Figure~\ref{fig:prepanasyn} shows the execution
order from top to bottom, and the data passed between functions in
coloured branches.

\begin{figure}[b]
  \begin{center}
    \input{figures/prepanasyn.pdftex_t}
    \caption{Execution flow and result propagation in a method.
      \texttt{prepare()} is executed first, and its return value is
      available to both the \texttt{analysis()} and
      \texttt{synthesis()} functions.  There are \texttt{slices} (a
      configurable parameter) number of parallel \texttt{analysis()}
      processes, and their outputs are available to the
      \texttt{synthesis()} function, which is the last to execute.}
    \label{fig:prepanasyn}
  \end{center}
\end{figure}





\clearpage

\section{Datasets: Storing Data}

The \texttt{dataset} is the Accelerator's default storage structure
for small or large quantities of data, designed for parallel
processing and high performance.  Datasets are built on top of jobs,
so \emph{datasets are created by methods and stored in job
directories.}

Internally, data in a dataset is stored in a row-column format.  It is
typically \emph{sliced} into a fixed number of slices to allow
efficient parallel access, both for reading and writing, see
figure~\ref{fig:dataset}. The different columns in a dataset are
stored and accessed independently, so there is no overhead in reading
just a single or a smaller set of all columns of a dataset.  Only the
data relevant for a task is read from disk.

\begin{figure}[h!]
  \begin{center}
    \vspace{3ex}
    \input{figures/dataset_files.pdftex_t}
    \vspace{1ex}
    \caption{A dataset containing three columns, $A$, $B$, and $C$,
      stored using two slices.  In this example, slice 0 stores the
      even rows, and slice 1 stores the odd rows.  Each dotted box
      corresponds to a file, so there are two files for each column,
      allowing for parallel read of the data using two independent
      processes.}
    \label{fig:dataset}
  \end{center}
\end{figure}

Furthermore, datasets may be \textsl{hash partitioned}, so that
slice-membership is based on the output of a hash function applied to
the values of a given column.  Hash partitioning is a very fast linear
time operation that is used to separate data so that parallel
computations can be carried out \emph{independently} in each slice,
without the need for complicated merging operations.  This is another
reason for the Accelerators high performance, and it is further
explained in section~\ref{sec:slicing_and_hashing}.



\subsection{Importing Existing Data from a File}

A project typically starts with \textsl{importing} some data from a
file on disk.  The bundled method \texttt{csvimport} is designed to
parse a plethora of ``comma separated values''-file formats and store
the data as a \emph{dataset}.
The method takes several input options in addition to the mandatory
filename to control the import process.  Here is an example
invocation
\begin{python}
def main(urd):
    jid = urd.build('csvimport', filename='file0.txt')
\end{python}

\begin{figure}[b]
  \begin{center}
    \input{figures/import_file1.pdftex_t}
    \caption{Importing \texttt{file0.txt}.  Unless a specific name is
      given, the name of the dataset will be \texttt{default}.  The
      dataset can later be referenced by \texttt{imp-0/default}, or,
      since there is only one dataset in \texttt{imp-0}, by the name
      of the job that created it: \texttt{imp-0}.}
    \label{fig:dataset_csvimport}
  \end{center}
\end{figure}

When executed, the created dataset will be stored in the resulting job
directory, and the name of the dataset will by default be the jobid
plus the string \texttt{default}.  For example, if the
\texttt{csvimport} jobid is \texttt{imp-0}, the dataset will be
referenced by \texttt{imp-0/default}.  In this case, the common case
where a job only contains one dataset, the dataset could also be
referenced by the job name \texttt{imp-0} for simplicity.  But in
general a job can contain any number of datasets and then a unique
name is required for each dataset.  See
figure~\ref{fig:dataset_csvimport}.


\clearpage

\subsection{Linking Datasets, Chaining}

Just like jobs can be linked to each other, datasets can link to each
other too.  Since datasets are built on top of jobs, this is
straightforward.  Assume the file \texttt{file0.txt} is imported into
dataset \texttt{imp-0/default}, and that there is more data like it
stored in the file \texttt{file1.txt}.  The second file is imported
with a link to the first dataset, see
figure~\ref{fig:dataset_csvimport_chain}.
\begin{figure}[t]
  \begin{center}
    \input{figures/import_file0file1.pdftex_t}
    \caption{Chaining the import of \texttt{file1.txt} to the previous
      import of \texttt{file0.txt}.}
    \label{fig:dataset_csvimport_chain}
  \end{center}
\end{figure}
The \texttt{imp-1} (or \texttt{imp-1/default}) dataset reference can
now be used to access all data imported from \textsl{both} files!

Linking datasets containing related content is called \emph{chaining},
and this is particularly convenient when dealing with data that grows
over time.  A good example is any kind of \emph{log} data, such as
logs of transactions, user interactions, and similar.  Using chaining,
datasets can be with more rows just by linking, which is a lightweight
constant time operation.



\subsection{Adding New Columns to a Dataset}
In the previous section it was shown that datasets can be chained and
thereby grow in number of rows.  A dataset chain is created simply by
linking one dataset to the other, so the overhead is minimal.  In this
section it is shown that by the same principles, it is equally simple
to add new columns to existing datasets.  Adding columns is a common
operation and the Accelerator handles this situation efficiently using
links.

The idea is very simple.  Assume a ``source'' dataset to which one or
more new columns should be added.  A new dataset is created containing
\textsl{only} the new column(s), and while creating it, the
constructor is instructed to link all the source dataset's columns to
the new dataset such that the new dataset appears to contain all
columns from both datasets.  (Note that this linking is similar to but
different from chaining.)

Accessing the new dataset will transparently access all the columns in
both the new and the source dataset in parallel, making it
indistinguishable from a single dataset.  See
Figure~\ref{fig:dep_dataset_append_column}.

\begin{figure}[b]
  \begin{center}
    \input{figures/dataset_append_column.pdftex_t}
    \caption{Adding one new column to the source dataset.}
    \label{fig:dep_dataset_append_column}
  \end{center}
\end{figure}

A common case is to compute new columns based on existing ones.  In
this case, values are written to the new columns in the new dataset
while reading from the iterator iterating over the existing columns in
the source dataset.  This will be discussed in detail in
section~\ref{sec:appending_new_columns}



\subsection{Multiple Datasets in a Job}

Typically, a method creates a single dataset in the job directory, but
there is no limit to how many datasets that could be created and
stored in a single job directory.  This leads to some interesting
applications.

One application for keeping multiple datasets in a job is when data is
split into subsets based on some condition.  This could, for example,
be when a dataset is split into a training set and a test set.  One
way to achieve this using the Accelerator is by creating a Boolean
column that tells if the current row is train or test data, followed
by a job that splits the dataset in two based on the value on that
column.  See Figure~\ref{fig:dep_dataset_csvimport_chain}.

\begin{figure}[h!]
  \hspace{1cm}
  \input{figures/filter_dataset.pdftex_t}
  \caption{\texttt{job-1} separates the dataset
    \texttt{job-0/default} into two new datasets, named
    \texttt{job-1/train} and \texttt{job-1/test}.}
  \label{fig:dep_dataset_csvimport_chain}
\end{figure}

In the setup of figure~\ref{fig:dep_dataset_csvimport_chain} we have
full tracking from either \texttt{train} or \texttt{test} datasets.
If we want to know the source of one of these sets, we just follow the
links back to the previous jobs until we reach the source job.  In the
figure, \texttt{job-0} may for example be a \texttt{csvimport} job,
and will therefore contain the name of the input file in its
parameters.  Thus, it is straightforward to link any data to its
source.

Splitting a dataset into parts creates ``physical'' isolation while
still keeping all the data at the same place.  No data is lost in the
process, and this is good for transparency reasons.  For example, a
following method may iterate over \textsl{both} datasets in
\texttt{job-1} and by that read the complete dataset.



\subsection{Parallel Dataset Access and Hash Partitioning}
As shown earlier in this chapter, data in datasets is stored in
multiple files for two reasons.  The first reason is that we can read
only the columns that we need, without overhead, and the second reason
is that it allows fast parallel reads.  The parameter \texttt{slices}
determines how many slices that the dataset should be partitioned
into, and it also sets the number of parallel processes that are used for
processing the dataset.  There is always one process for each slice of
the dataset, and each process operates on a unique part of the
dataset.

Datasets can be partitioned, or sliced, in different ways.  One
obvious way is to use round robin, where each consecutive data row is
written to the next slice, modulo the number of slices.  This leads to
``well balanced'' datasets with approximately equal number of rows per
slice.  Another alternative to slicing is to slice based on the hash
value of a particular column's values.  Using this method, all rows
with the same value in the hash column end up in the same slice.  This
is efficient for many parallel processing tasks, and will be described
in detail later on.




\subsection{Dataset Column Types}

There are large a number of useful types available for dataset
columns.  They include \textsl{floating} and \textsl{integer point
  numbers}, \textsl{Booleans}, \textsl{timestamps}, several
\textsl{string types} (handling all kinds of string encodings), and
\textsl{json} as well as \textsl{pickle} types for storing arbitrary
data collections.  Most of these types come with advanced parsers,
making importing data from text files straightforward with
deterministic handling of errors, overflows, and so on.



\subsection{Dataset Attributes}
The dataset has a number of attributes associated with it, such as
shape, number of rows, column names and types, and more.
An attribute is accessed like this
\begin{python}
datasets = ('source',)
def synthesis():
    print(datasets.source.shape)
    print(datasets.source.columns)
\end{python}
and so on.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Iterators: Working with Data}

Data in a dataset is typically accessed using an \emph{iterator} that
reads and streams one dataset slice at a time to a CPU core.  The
parallel processing capabilities of the Accelerator makes it possible
to dispatch a set of parallel iterators, one for each slice, in order
to have efficient parallel processing of the whole dataset.

This section shows how iterators are used for reading data, how to
take advantage of slicing to have parallel processing, and how to
efficiently create new datasets.


\subsection{Iterator Basics}

Basic iterator functionality is introduced here using an example.
Assume a dataset that has a column containing movie titles named
\texttt{movie}, and the problem is to extract the ten most frequently
occuring movies.  Here's a complete method solving this problem
\begin{python}
from collections import Counter
datasets = ('source',)

def synthesis():
    c = Counter(datasets.source.iterate(None, 'movie'))
    top10 = c.most_common(10)
    print(top10)
    return top10
\end{python}
This will print the ten most common movie titles and their
corresponding counts in the \texttt{source} dataset.  The code will
run on a single CPU core, because we use the single-process
\texttt{synthesis()} function, which is called and executed only once.
The \texttt{dataset.iterate()} (class-)method therefore has to read
through all slices, one at a time, in a serial fashion, and this is
reflected by the first argument to the iterator being \pyNone.  The
method also returns the variable \texttt{top10} so that it can be used
by other methods.



\subsection{Parallel Execution}
It is easy to write parallel programs using the Accelerator.  The fact
that data in a dataset is sliced into disjoint sets and files makes
parallel processing of data straightforward.  Here's a slightly
modified version of the program from the previous section that will
now execute in parallel
\begin{python}
def analysis(sliceno):
    return Counter(datasets.source.iterate(sliceno, 'movie'))

def synthesis(analysis_res):
    c = analysis_res.merge_auto()
    top10 = c.most_common(10)
    return top10
\end{python}
For larger datasets, this parallel version of the movie title counter
will run much faster.  Here, \texttt{.iterate()} is moved inside the
\texttt{analysis()} function.  This function is forked once for each
slice, and the argument \texttt{sliceno} will contain an integer
between zero and the number of slices minus one.  The returned value
from the analysis functions will be available as input to the
synthesis function in the \texttt{analysis\_res} Python iterable.  It
is possible to merge the results explicitly, but the iterable
comes with a rather magic method \texttt{merge\_auto()}, that merges
the results from all slices into one based on the data type.  It can
for example merge \texttt{Counter}s, \texttt{set}s, and composed types
like \texttt{set}s of \texttt{Counter}s, and so on.


\subsection{Iterating over Several Columns}
Since each column is stored independently in a dataset, there is no
overhead from reading a subset of a dataset's columns.  In the
previous section we've seen how to iterate over a single column using
\texttt{.iterate()}.  Iterating over more columns is straightforward
by feeding a list of column names to \texttt{.iterate()}, like in this
example
\begin{python}
from collections import defaultdict
datasets = ('source',)

def analysis(sliceno):
    user2movieset = defaultdict(set)
    for user, movie in datasets.source.iterate(sliceno, ('user', 'movie')):
        user2movieset[user].add(movie)
    return user2movieset
\end{python}
This example creates a lookup dictionary from users to sets of movies.
Note that in this case, we would like to have the dataset hashed on
the \texttt{user} column, so that each user appears in exactly one slice.
This will make later merging (if necessary) much easier.

It is also possible to iterate over all columns by specifying an empty
list of columns or by using the value \pyNone.
\begin{python}
...
def analysis(sliceno):
    for columns in datasets.source.iterate(sliceno, None):
        ...
\end{python}
Here, \texttt{columns} will be a list of values, one for each column
in the dataset, in sorted column name order.


\subsection{Iterating over Dataset Chains}

The \texttt{iterate} function is used to iterate over a single
dataset.  There is a corresponding function, \texttt{iterate\_chain},
that is used for iterating over chains of datasets.  This function
takes a number of arguments, such as
\begin{itemize}
\item[] \texttt{length}, i.e.\ the number of datasets to iterate over.
  By default, it will iterate over all datasets in the chain.
\item[] \texttt{callbacks}, functions that can be called before and/or
  after each dataset in a chain.  Very useful for aggregating data
  between datasets.
\item[] \texttt{stop\_id} which stops iterating at a certain dataset.
  This dataset could be from \textsl{another} job's parameters, so we
  can for example iterate exactly over all new datasets not covered by
  a previous job.
\item[] \texttt{range}, which allows for iterating over a range of
  data.
\end{itemize}
The \texttt{range} options is based on the max/min values stored for
each column in the dataset.  Assuming that the chain is sorted, one
can for example set
\begin{python}
range={'ts_column': ('2016-01-01', '2016-01-31')}
\end{python}
in order to get rows within the specified range only.  Using
\texttt{range=} is quite costly, since it requires each row in the
dataset chain with dates within the range to be checked against the
range criterion.  Therefore, there is a \texttt{sloppy} version that
iterates over complete datasets in the chain that contains at least
one row with a date within the range.  This runs at full speed, and is
useful, for example, to very quickly produce histograms or plots of
subsets of a huge dataset.



\subsection{Job Execution Flow and Result Passing}

Execution of code in a method is either parallel or serial depending
on which function is used to encapsulate it.  There are three
functions in a method that are called from the Accelerator when a
method is running, and they are \texttt{prepare()},
\texttt{analysis()}, and \texttt{synthesis()}.  All three may exist in
the same method, and at least one is required.  When the method
executes, they are called one after the other.
\begin{itemize}
\item[] \texttt{prepare()} is executed first.  The returned value is
  available in the variable \texttt{prepare\_res}.
\item[] \texttt{analysis()} is run in parallel processes, one for each
  slice.  It is called after completion of, and actually forked from
  \texttt{prepare()}.  Common input parameters are \texttt{sliceno},
  holding the number of the current process instance, and
  \texttt{prepare\_res}.  The return value for each process becomes
  available in the \texttt{analysis\_res} variable.
\item[] \texttt{synthesis()} is called after the last
  \texttt{analysis()}-process is completed.  It is typically used to
  aggregate parallel results created by \texttt{analysis()} and takes
  both \texttt{prepare\_res} and \texttt{analysis\_res} as optional
  parameters.  The latter is an iterator of the results from the
  parallel processes.
\end{itemize}
Figure~\ref{fig:prepanasyn} shows the execution order from top to
bottom, and the data passed between functions in coloured branches.
\texttt{prepare()} is executed first, and its return value is
available to both the \texttt{analysis()} and \texttt{synthesis()}
functions.  There are \texttt{slices} (a configurable parameter)
number of parallel \texttt{analysis()} processes, and their output is
available to the \texttt{synthesis()} function, which is executed
last.

Return values from any of the three functions may be stored in the
job's directory making them available to other jobs.


\begin{figure}[t]
  \begin{center}
    \input{figures/prepanasyn.pdftex_t}
    \caption{Execution flow and result propagation in a method.}
    \label{fig:prepanasyn}
  \end{center}
\end{figure}


\begin{figure}[b]
  \begin{center}
    \input{figures/execflow.pdftex_t}
    \caption{Execution flow of a method.  The method takes optionally
      three kinds of parameters: \texttt{options}, \texttt{jobs},
      and \texttt{datasets}.}
    \label{fig:execflow}
  \end{center}
\end{figure}


\clearpage
\subsection{Job Parameters}
\label{sec:jobparams}
We've seen how completed jobs can be used as input to new
jobs.  Jobs are one of three kinds of input parameters that
a job can take.  Here the input parameters are summarised:
\begin{itemize}
\item[] \texttt{jobs}, a set, or list, of identifiers to previously executed jobs;
\item[] \texttt{options}, a dictionary of options; and
\item[] \texttt{datasets}, a set, or list, of input \textsl{datasets}.
\end{itemize}
See Figure~\ref{fig:execflow}.  Parameters are entered as global
variables early in the method's source.


\section{A Class Based Programming Model}
The Accelerator is based on an class based paradigm. Access to the
Accelerator's built in functions and parameters are typically done
through a few \textsl{objects} that are populated by the running
Accelerator.
