%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                                                          %
% Copyright (c) 2018 eBay Inc.                                             %
% Modifications copyright (c) 2019-2021 Anders Berkeman                    %
%                                                                          %
% Licensed under the Apache License, Version 2.0 (the "License");          %
% you may not use this file except in compliance with the License.         %
% You may obtain a copy of the License at                                  %
%                                                                          %
%  http://www.apache.org/licenses/LICENSE-2.0                              %
%                                                                          %
% Unless required by applicable law or agreed to in writing, software      %
% distributed under the License is distributed on an "AS IS" BASIS,        %
% WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. %
% See the License for the specific language governing permissions and      %
% limitations under the License.                                           %
%                                                                          %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%





\section{High Level View}
The Accelerator is a client-server based application, and from a high
level, it can be visualised like in figure~\ref{fig:overview}.

\begin{figure}[h!]
  \begin{center}
    \vspace{2ex}
    \input{figures/overview.pdftex_t}
    \caption{High level view of the Accelerator framework.  See text
      for details.}
    \label{fig:overview}
  \end{center}
\end{figure}

The left part of the figure shows the \emph{client} side, with shell
commands and web interface.  On the right side is the \emph{server}
and its databases.  The Accelerator executes \textsl{build scripts},
that start \textsl{jobs} on the server.  These jobs are stored in the
server's job databases for later retreival.  There are two databases,
one for storing everything related to a job's execution, called the
\textsl{workdir}, and one transaction log used for storing
meta-information about built jobs.  The workdir will contain all
inputs, source code, parameters, and outputs of all executed jobs,
whereas the job log database ensures reproducibility and transparency,
and it will be further discussed in chapter~\ref{chap:urd}.


\section{Interacting with the Accelerator}
There are two ways to interface the Accelerator, using
\begin{itemize}
\item[] the command line, and
\item[] using a web brower.
\end{itemize}
The command line interface is based on the \texttt{ax} shell command.
It can be used for a large variety of operations, including starting
execution of scripts, listing or grepping data in datasets, finding
particular jobs, and a lot more.

The web interface, called \textsl{Accelerator Board}, is used to
inspect jobs, datasets, and meta-information.  Resulting files such as
images, text files, pdfs, video files and so on are rendered in the
web browser.


\section{Jobs}
A problem that is solved using the Accelerator is typically
partitioned into smaller programs called \emph{methods}.  A method is
a Python program with one or a few special functions used to execute
code sequentially or in parallel, and to pass parameters and results.

For each method that is executed by the Accelerator, a unique new
directory, called \emph{job directory}, is created in which everything
related to the execution, such as inputs, outputs, source code etc.,
is stored.  An executed method is called a \emph{job}.  To be truly
transparent, the job directory will also contain profiling
information, Python interpreter version and other related information.

The Accelerator has a database that keeps track of all jobs that has
been run.  This avoids unnecessary re-computing in favour of reusing
previously computed results.  Reusing jobs does not only speed up
processing and encourage incremental design, but also makes it
transparent which code and which data that was used for any particular
result, minimising these kind of uncertainties to zero.


\subsection{A ``Hello, World'' Example}
Here's an example of a very simple method.  It does not take any input
parameters and does almost nothing, it will just return the string
``\texttt{hello world}'' and exit.  The example is stored in a file
named \texttt{a\_hello\_world.py}\footnote{There's a good reason for
the file name prefix, see section~\ref{@@@@@ Mention that all methods are imported/reloaded from time to time and this might cause side effects if any random program is executed, and there might be broken scripts too @@@@@@@}.}:
\begin{python}
def synthesis():
    return "hello world"
\end{python}
The \texttt{synthesis()}-function is reserved for code that is to be
executed once without parallellisation.  The method is stored in a
separate file.  In order to get the method to execute, it is called
from a \textsl{build script} looking something like this
\begin{python}
def main(urd):
    job = urd.build('hello_world')
    print(job.load())
\end{python}
and stored in a file \texttt{build\_mytest.py}.  The
\texttt{main()}-function is called when the build script is run.  The
input parameter \texttt{urd} is an object that contains functions for
job building and organisation, and is described in
chapter~\ref{chap:urd} and section~\ref{sec:classes:urd}.  The build
script is executed by typing a shell command like this
\begin{shell}
ax run mytest
\end{shell}
In the build script, the \texttt{urd.build()}-call will instruct the
server to execute the \texttt{hello\_world} method.  During
its execution, a job directory will be created that contains
everything associated with the build process.  When the
\texttt{build()}-call is completed, a job object, of type
\texttt{Job}, is returned to the program.  This object provides a
convenient interface to the data in the corresponding job directory,
and contains member functions such as \texttt{.load()}, that is used
in the example to read back the returned value from the job.

Note that what is returned by the method is stored in a file on disk.
There is no need to know the name of the file, since its contents can
be retrieved by the job objects \texttt{load()} function.



\subsection{Jobs Can Only be Run Once}
If the build script is executed again, the \texttt{hello\_world} job
will \emph{not} be re-built, because the Accelerator remembers that
the job has been built in the past, and all associated information is
already stored in a job directory.  Instead, the Accelerator
immediately returns a job object representing the previous run.  This
means that from a user's perspective, there is no visible difference
between running a job for the first time or re-using results from an
existing run!  In order to have the method execute again, either its
source code or input parameters has to change.  If there are such
changes, the method will be re-executed, and a new job will be created
that reflects these changes.


\begin{figure}[b!]
  \hspace{1.5cm}\input{figures/job0.pdftex_t}
    \caption{A simple hello world program, represented as graph and
      job directory.  Building the \texttt{hello\_world} method
      creates a \emph{job} \texttt{test-0}.  This job is stored in a
      (job-) directory, using the same name, in the \texttt{test}
      \emph{workdir}.  Everything associated with the build is stored
      in this job directory.}
    \label{fig:execflow-hello-world}
\end{figure}


\clearpage


\subsection{A Quick Look Under the Hood}
The Accelerator is fully transparent, and no data is ``hidden''
anywere.  There is no need to know how or where things are stored to
use the Accelerator, but still it could be of interest to know where
things ends up (and that it is straightforward to access things using
simple shell commands.)

Figure~\ref{fig:execflow-hello-world} illustrates an execution of the
\texttt{hello\_world} method.  The created job gets the \textsl{jobid}
\texttt{test-0}, and parts of the corresponding job directory
information is shown in green.  Jobids are job identifiers, named by
their corresponding \textsl{workdir} plus an integer counter value.


The most important files in a job directory are
\begin{itemize}
\item[] \texttt{setup.json}, containing job meta information;
\item[] \texttt{result.pickle}, containing the returned data; and
\item[] \texttt{method.tar.gz}, containing the method's source code,
\end{itemize}
and in general the job directory could contain any number of files.

The \texttt{Job} object, which is returned by the build call, provides
a convenient way to access files and data stored in the job directory.
For example, the job's return value can be loaded into a variable
using the \texttt{.load()} function, all metadata can be pretty
printed using the \texttt{ax job} shell command, and so on.


\subsection{Workdirs and Sharing Jobs}

A workdir is where jobs are stored.  There can be more than one
workdir, and different workdirs can be used separate jobs into
different physical locations, or to separate different types of data
from one another in the same project.  The Accelerator can be set up
to have any number of workdirs accessible, but typically only one is
used for writing.

Entering the same workdir definition into two or more users'
configuration files, the workdir and its contents will be shared
between the users.  Each Accelerator server will update its knowledge
about the contents of all workdirs before executing a build script, to
make sure that the latest jobs are taken into account.  In addition,
the job log database, as described in chapter~\ref{chap:urd}, is
designed for efficient re-use and sharing of particularly interesting
jobs.  Thus, only one user has to execute a given method for all other
users to have instant access to it.  This can save a lot of time in a
project.


\begin{figure}[b]
    \hspace{1.5cm}\input{figures/job0job1.pdftex_t}
    \caption{Job \texttt{test-0}, is used as input to the
      \texttt{test-1} job.  Jobs are created from methods
      (i.e.\ source code), input parameters, and processing time using
      the \texttt{build()} call.  Each job corresponds to a number of
      files stored in a per job unique (job) directory.}
    \label{fig:execflow-print-result}
\end{figure}




\subsection{Linking Jobs}

Using jobs, complex tasks can be split into several smaller
operations.  Jobs can be connected to eachother so that a new job will
use results from one or more previously executed jobs.

To continue the simple hello world example, assume for a second that
the \texttt{hello\_world}-job is computationally expensive, and should
therefore be run as few times as possible to save time and energy.
The job creates a result that will be used in further processing.
This further processing is represented in this example by printing the
result to standard output.

\clearpage

\noindent A new method \texttt{print\_result} is
created for this task, and it looks like this
\begin{python}
jobs = ('hello_world_job',)

def synthesis():
    data = jobs.hello_world_job.load()
    print(data)
\end{python}
The ``\texttt{jobs=}''-line at the top specifies that the method
expects a reference to a job to be provided as input at run time.  The
input is arbitrarily named \texttt{hello\_world\_job}, and the
variable \texttt{jobs.hello\_world\_job} will be of type \texttt{Job}, and
therefore its result can be read using the \texttt{.load()} function
as shown previously.

The following extended build script will run both the first and second
method, and provide the first job as input to the second:
\begin{python}
def main(urd):
    job1 = urd.build('hello_world')
    job2 = urd.build('print_result', hello_world_job=job1)
\end{python}

Figure~\ref{fig:execflow-print-result} illustrates the situation.
Note the direction of the arrow.  Arrows illustrate pointers to
previous jobs.  In this example, the second job, \texttt{test-1}, has
\texttt{test-0} as input parameter, i.e.\ \texttt{test-1} has a
pointer to \texttt{test-0}.  (And \texttt{test-0} does not know of any
jobs run in the future, so the arrow cannot point out from it.)

The example shows how a potentially complex task can be split into
several jobs, each reading intermediate results from previous ones,
\emph{without the need for explicit filenames}!  The Accelerator will
keep track of all job dependencies, so there is no doubt which jobs
that are run when, and on which data.  And since the Accelerator
remembers if a job has been executed before, it will link and
\emph{re-use previous jobs when possible}, thereby saving time and
energy.

Also note that a re-used job is a \emph{proof} of that the code,
input- and output data is unchanged and connected.  \emph{Using the
Accelerator, there is no risk that wrong intermediate files are used
by mistake}, since the files are accessed by reference to the method
that created them, and not by an arbitrary user-selected filenames!


\subsection{Job Input Parameters}
In the previous section it is shown how the input parameter \jobs is
used to point to earlier jobs.  There are two more types of input
parameters: \options and \datasets, and here's an example of how to
declare all of them in a method:
\begin{python}
jobs = ('hello_world_job',)   # as seen in the previous example
datasets = ('transaction_log', 'access_log',)
options = dict(length=4)
\end{python}
All paramteres are populated by the \texttt{build()} call in the build
script.  The \jobs parameter is used to link the job to previous jobs.
\options is a dict of any kind of options the job may use, and
\datasets is, similar to \jobs, a list of datasets.  (Datasets will be
introduced shortly.)


\subsection{Parallel Processing}

While the \texttt{synthesis()}-function (presented in the previous
section) is used for code that is to execute exactly once, the
\texttt{analysis()}-function is used for simple but powerful parallel
processing.  A method making use of parallel processin may look like
this
\begin{python}
def analysis(sliceno):
    print('This is process %d!' % (sliceno,))
\end{python}
When the method is executed, the \texttt{analysis()}-function will be
forked into \texttt{slices} parallel processes (where
\texttt{slices} is specified in the Accelerator's configuration file
and typically set to the number of available CPU cores of the
machine).  The input parameter \texttt{sliceno} is a number between
zero and \texttt{slices-1} and used to identify each process.

Admittedly, this is the most basic form of parallel processing, that
can only solve a subset of all kinds of parallel problems.
Nevertheless, it solves common data processing problems, and it solves
them in a very simple and efficient manner.  The next section shows
how the Accelerator makes use of the \texttt{analysis()}-function to
do data processing on all CPUs in parallel. @@


\subsection{Job Execution Flow}

Execution of code in a method is either parallel or serial depending
on which function is used to encapsulate it.  In addition to
\texttt{analysis()} and \texttt{synthesis()} that have already been
introduced, there is a third one named \texttt{prepare()}.  All three
may exist in the same method, and at least one is required.  When the
method executes, they are called one after the other.

Results can be transferred from one function to the next using return
values.  There is also an easy way to merge results from parallel
processing (introduced in section~\ref{jobs}).

\begin{itemize}
\item[] \texttt{prepare()} is executed first.  The returned value is
  available in the variable \texttt{prepare\_res}.
\item[] \texttt{analysis()} is run in \texttt{slices} parallel
  processes, forked after completion of \texttt{prepare()}.  Common
  input parameters are \texttt{sliceno}, holding the number of the
  current process instance, and \texttt{prepare\_res}.  The return
  value for \emph{all} processes becomes available in the
  \texttt{analysis\_res} generator variable.
\item[] \texttt{synthesis()} is called after the last
  \texttt{analysis()}-process is completed.  It is typically used to
  aggregate parallel results created by \texttt{analysis()} and takes
  both \texttt{prepare\_res} and \texttt{analysis\_res} as optional
  input parameters.
\end{itemize}
The return value from \texttt{synthesis()} is the default return value
of the job itself.  Figure~\ref{fig:prepanasyn} shows the execution
order from top to bottom, and the data passed between functions in
coloured branches.

\begin{figure}[b]
  \begin{center}
    \input{figures/prepanasyn.pdftex_t}
    \caption{Execution flow and result propagation in a method.
      \texttt{prepare()} is executed first, and its return value is
      available to both the \texttt{analysis()} and
      \texttt{synthesis()} functions.  There are \texttt{slices} (a
      configurable parameter) number of parallel \texttt{analysis()}
      processes, and their outputs are available to the
      \texttt{synthesis()} function, which is the last to execute.}
    \label{fig:prepanasyn}
  \end{center}
\end{figure}





\clearpage

\section{Datasets: Storing Data}

The \texttt{dataset} is the Accelerator's default storage structure
for small or large quantities of data, designed for parallel
processing and high performance.  Datasets are built on top of jobs,
so \emph{datasets are created by methods and stored in job
directories.}  To ensure data integrity, datasets are
\textsl{immutable}, i.e.\ their contents cannot be modified.

Internally, data in a dataset is stored in a row-column format.  It is
typically \emph{sliced} into a fixed number of slices to allow
efficient parallel access, both for reading and writing, see
figure~\ref{fig:dataset}. The different columns in a dataset are
stored and accessed independently, so there is no overhead in reading
just a single or a smaller set of all columns of a dataset.  Only the
data relevant for a task is read from disk.

\begin{figure}[h!]
  \begin{center}
    \vspace{3ex}
    \input{figures/dataset_files.pdftex_t}
    \vspace{1ex}
    \caption{A dataset containing three columns, $A$, $B$, and $C$,
      stored using two slices, i.e. \texttt{slices}=2.  Subscripts
      corresponds to line numbers.  In this example, slice 0 stores
      the even rows, and slice 1 stores the odd rows.  Each dotted box
      corresponds to a file, so columns could be read independently,
      and since there are two files for each column, any column could
      be processed in parallel using two independent processes.}
    \label{fig:dataset}
  \end{center}
\end{figure}

Data in a dataset is typically accessed using a Python \emph{iterator}
that reads and streams a dataset slice from disk or memory to a CPU
core for processing.  Here's a simplified example showing how to
create a count the frequency of each movie in the \texttt{movie}
column of a dataset.  More and complete examples will be presented
later in this chapter.

\begin{python}
from collections import Counter
c = Counter()
for movie in dataset.iterate(None, 'movie'):
    C[movie] += 1
\end{python}

The streaming iterator approach has several benefits.  It enables
working on unlimited sizes of data, since only a fraction of the data
is in memory at any given time.  Big data analysis can be carried out
with ease on a laptop.  Also, iterators can be really fast, and the
corresponding linear accessing of data on disk is the fastest way to
access harddrive data, since it avoids time wasting seek calls.





Furthermore, datasets may be \textsl{hash partitioned}, so that
slice-membership is based on the output of a hash function applied to
the values of a given column.  Hash partitioning is a very fast linear
time operation that is used to separate data so that parallel
computations can be carried out \emph{independently} in each slice,
without the need for complicated merging operations.  This is another
reason for the Accelerators high performance, and it is further
explained in section~\ref{sec:slicing_and_hashing}.

The parallel processing capabilities of the Accelerator makes
it possible to dispatch a set of parallel iterators, one for each
slice and CPU core, in order to maximise use of all available hardware
resources.

This section shows how iterators are used for reading data, how to
take advantage of slicing to have parallel processing, and how to
efficiently create new datasets.

For further information on datasets, visit Chapter \ref{chap:datasets}.



\subsection{Importing Existing Data from a File}

A project typically starts with \textsl{importing} some data from a
file on disk.  The bundled method \texttt{csvimport} is designed to
parse a plethora of ``comma separated values''-file formats and store
the data as a \emph{dataset}.
The method takes several input options in addition to the mandatory
filename to control the import process, see section~\ref{}  Here is an example
invocation
\begin{python}
def main(urd):
    jid = urd.build('csvimport', filename='file0.txt')
\end{python}

\begin{figure}[b]
  \begin{center}
    \input{figures/import_file1.pdftex_t}
    \caption{Importing \texttt{file0.txt}.  The ellipse denotes a job,
      and the inscribed rectangle denotes a dataset.  Unless a
      specific name is given, the name of the dataset will be
      \texttt{default}.  The dataset can later be referenced by
      \texttt{imp-0/default}, or, since there is only one dataset in
      \texttt{imp-0}, by the name of the job that created it:
      \texttt{imp-0}.}
    \label{fig:dataset_csvimport}
  \end{center}
\end{figure}

When executed, the created dataset will be stored in the resulting job
directory, and the name of the dataset will by default be the jobid
plus the string \texttt{default}.  For example, if the
\texttt{csvimport} jobid is \texttt{imp-0}, the dataset will be
referenced by \texttt{imp-0/default}.  In this case, the common case
where a job only contains one dataset, the dataset could also be
referenced by the job name \texttt{imp-0} for simplicity.  But in
general a job can contain any number of datasets and then a unique
name is required for each dataset.  See
figure~\ref{fig:dataset_csvimport}.



\subsection{Linking Datasets, Chaining}

Just like jobs can be linked to each other, datasets can link to each
other too.  Since datasets are built on top of jobs, this is
straightforward.  Assume the file \texttt{file0.txt} is imported into
dataset \texttt{imp-0/default}, and that there is more data like it
stored in the file \texttt{file1.txt}.  The second file is imported
with a link to the first dataset, see
figure~\ref{fig:dataset_csvimport_chain}.
\begin{figure}[t]
  \begin{center}
    \input{figures/import_file0file1.pdftex_t}
    \caption{Chaining the import of \texttt{file1.txt} to the previous
      import of \texttt{file0.txt}.}
    \label{fig:dataset_csvimport_chain}
  \end{center}
\end{figure}
The \texttt{imp-1} (or \texttt{imp-1/default}) dataset reference can
now be used to access all data imported from \textsl{both} files!

Linking datasets containing related content is called \emph{chaining},
and this is particularly convenient when dealing with data that grows
over time.  A good example is any kind of \emph{log} data, such as
logs of transactions, user interactions, and similar.  Using chaining,
new data can be added and associated with already existing data just
by linking, which is a very lightweight constant time operation.

For further information on how to manually create the said datasets,
visit Section \ref{sec:datasetwriter}.



\subsection{Adding New Columns to a Dataset}
In the previous section it was shown that datasets can be chained and
thereby grow in number of rows.  A dataset chain is created simply by
linking one dataset to the other, so the overhead is minimal.  Based
on the same principles, it is equally simple to add new columns to
existing datasets.  Adding columns is a common operation and the
Accelerator handles this situation efficiently using links.

The idea is very simple.  Assume a ``source'' dataset to which one or
more new columns should be added.  A new dataset is created containing
\textsl{only} the new column(s), and while creating it, the
constructor is instructed to link all the source dataset's columns to
the new dataset such that the new dataset appears to contain all
columns from both datasets.  (Note that this linking is similar to but
different from chaining.)

Accessing the new dataset will transparently access all the columns in
both the new and the source dataset in parallel, making it
indistinguishable from a single dataset.  See
Figure~\ref{fig:dep_dataset_append_column}.

\begin{figure}[b]
  \begin{center}
    \input{figures/dataset_append_column.pdftex_t}
    \caption{Adding one new column to the source dataset.}
    \label{fig:dep_dataset_append_column}
  \end{center}
\end{figure}

New columns are typically based on already existing ones.  Then,
values are written to the new columns in the new dataset while reading
(iterating) from the existing columns in the source dataset.  This
will be discussed in detail in section~\ref{sec:appending_new_columns}



\subsection{Multiple Datasets in a Job}

Typically, a method creates a single dataset in the job directory, but
there is no limit to how many datasets that could be created and
stored in a single job directory.  This flexibility opens up for a
number of compact and efficient solutions to various common problems.

For example, one application for keeping multiple datasets in a job is
the \texttt{csvimport\_zip} method, which reads compressed zip
archives and runs \texttt{csvimport} on each of the included files.
Each file is turned into a dataset, and thus there will one dataset in
the job per file in the archive.

@@marker

%% One application for keeping multiple datasets in a job is when data is
%% split into subsets based on some condition.  This could, for example,
%% be when a dataset is split into a training set and a test set.  One
%% way to achieve this using the Accelerator is by creating a Boolean
%% column that tells if the current row is train or test data, followed
%% by a job that splits the dataset in two based on the value on that
%% column.  See Figure~\ref{fig:dep_dataset_csvimport_chain}.

%% \begin{figure}[h!]
%%   \hspace{1cm}
%%   \input{figures/filter_dataset.pdftex_t}
%%   \caption{\texttt{job-1} separates the dataset
%%     \texttt{job-0/default} into two new datasets, named
%%     \texttt{job-1/train} and \texttt{job-1/test}.}
%%   \label{fig:dep_dataset_csvimport_chain}
%% \end{figure}

%% In the setup of figure~\ref{fig:dep_dataset_csvimport_chain} we have
%% full tracking from either \texttt{train} or \texttt{test} datasets.
%% If we want to know the source of one of these sets, we just follow the
%% links back to the previous jobs until we reach the source job.  In the
%% figure, \texttt{job-0} may for example be a \texttt{csvimport} job,
%% and will therefore contain the name of the input file in its
%% parameters.  Thus, it is straightforward to link any data to its
%% source.

%% Splitting a dataset into parts creates ``physical'' isolation while
%% still keeping all the data at the same place.  No data is lost in the
%% process, and this is good for transparency reasons.  For example, a
%% following method may iterate over \textsl{both} datasets in
%% \texttt{job-1} and by that read the complete dataset.



\subsection{Parallel Dataset Access and Hash Partitioning}
As shown earlier in this chapter, data in datasets is stored in
multiple files.  Columns are stored independently, and each column is
\textsl{sliced} into \texttt{slices} independent files.  In this
fashion, columns can be read independently without overhead, and
multiple CPUs can operate on different files/slices from the same
column(s) at the same time.
@@@@
Datasets can be partitioned, or sliced, in different ways.  One
obvious way is to use round robin, where each consecutive input data
row is written to the next slice, modulo the total number of slices.
This leads to ``well balanced'' datasets with approximately equal
number of rows per slice.

@@@@
Another interesting slicing alternative is
to slice based on a \textsl{hash function} applied to a particular
column's values.  Using this method, all rows with the same hash value
in the hashed column end up in the same slice.
@@@@



\subsection{Dataset Column Types}

There are large a number of useful types available for dataset
columns.  They include \textsl{floating} and \textsl{integer point
  numbers}, \textsl{Booleans}, \textsl{timestamps}, several
\textsl{string types} (handling all kinds of string encodings), and
\textsl{json} as well as \textsl{pickle} types for storing arbitrary
data collections.  Most of these types come with advanced parsers,
making importing data from text files straightforward with
deterministic handling of errors, overflows, and so on.



\subsection{Dataset Attributes}
The dataset has a number of attributes associated with it, such as
shape, number of rows, column names and types, and more.
An attribute is accessed like this
\begin{python}
datasets = ('source',)
def synthesis():
    print(datasets.source.shape)
    print(datasets.source.columns)
\end{python}
and so on.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Iterators: Working with Data}

Data in a dataset is typically accessed using an \emph{iterator} that
reads and streams one dataset slice at a time to a CPU core.  The
parallel processing capabilities of the Accelerator makes it possible
to dispatch a set of parallel iterators, one for each slice, in order
to have efficient parallel processing of the whole dataset.

This section shows how iterators are used for reading data, how to
take advantage of slicing to have parallel processing, and how to
efficiently create new datasets.


\subsection{Iterator Basics}

Basic iterator functionality is introduced here using an example.
Assume a dataset that has a column containing movie titles named
\texttt{movie}, and the problem is to extract the ten most frequently
occuring movies.  Here's a complete method solving this problem
\begin{python}
from collections import Counter
datasets = ('source',)

def synthesis():
    c = Counter(datasets.source.iterate(None, 'movie'))
    top10 = c.most_common(10)
    print(top10)
    return top10
\end{python}
This will print the ten most common movie titles and their
corresponding counts in the \texttt{source} dataset.  The code will
run on a single CPU core, because we use the single-process
\texttt{synthesis()} function, which is called and executed only once.
The \texttt{dataset.iterate()} (class-)method therefore has to read
through all slices, one at a time, in a serial fashion, and this is
reflected by the first argument to the iterator being \pyNone.  The
method also returns the variable \texttt{top10} so that it can be used
by other methods.



\subsection{Parallel Execution}
It is easy to write parallel programs using the Accelerator.  The fact
that data in a dataset is sliced into disjoint sets and files makes
parallel processing of data straightforward.  Here's a slightly
modified version of the program from the previous section that will
now execute in parallel
\begin{python}
def analysis(sliceno):
    return Counter(datasets.source.iterate(sliceno, 'movie'))

def synthesis(analysis_res):
    c = analysis_res.merge_auto()
    top10 = c.most_common(10)
    return top10
\end{python}
For larger datasets, this parallel version of the movie title counter
will run much faster.  Here, \texttt{.iterate()} is moved inside the
\texttt{analysis()} function.  This function is forked once for each
slice, and the argument \texttt{sliceno} will contain an integer
between zero and the number of slices minus one.  The returned value
from the analysis functions will be available as input to the
synthesis function in the \texttt{analysis\_res} Python iterable.  It
is possible to merge the results explicitly, but the iterable
comes with a rather magic method \texttt{merge\_auto()}, that merges
the results from all slices into one based on the data type.  It can
for example merge \texttt{Counter}s, \texttt{set}s, and composed types
like \texttt{set}s of \texttt{Counter}s, and so on. It cannot, however,
merge virtually any type. See the documentation in the code for the whole
list of supported merges.


\subsection{Iterating over Several Columns}
Since each column is stored independently in a dataset, there is no
overhead from reading a subset of a dataset's columns.  In the
previous section we've seen how to iterate over a single column using
\texttt{.iterate()}.  Iterating over more columns is straightforward
by feeding a list of column names to \texttt{.iterate()}, like in this
example
\begin{python}
from collections import defaultdict
datasets = ('source',)

def analysis(sliceno):
    user2movieset = defaultdict(set)
    for user, movie in datasets.source.iterate(sliceno, ('user', 'movie')):
        user2movieset[user].add(movie)
    return user2movieset
\end{python}
This example creates a lookup dictionary from users to sets of movies.
Note that in this case, we would like to have the dataset hashed on
the \texttt{user} column, so that each user appears in exactly one slice.
This will make later merging (if necessary) much easier.

It is also possible to iterate over all columns by specifying an empty
list of columns or by using the value \pyNone.
\begin{python}
...
def analysis(sliceno):
    for columns in datasets.source.iterate(sliceno, None):
        ...
\end{python}
Here, \texttt{columns} will be a list of values, one for each column
in the dataset, in sorted column name order.


\subsection{Iterating over Dataset Chains}

The \texttt{iterate} function is used to iterate over a single
dataset.  There is a corresponding function, \texttt{iterate\_chain},
that is used for iterating over chains of datasets.  This function
takes a number of arguments, such as
\begin{itemize}
\item[] \texttt{length}, i.e.\ the number of datasets to iterate over.
  By default, it will iterate over all datasets in the chain.
\item[] \texttt{callbacks}, functions that can be called before and/or
  after each dataset in a chain.  Very useful for aggregating data
  between datasets.
\item[] \texttt{stop\_id} which stops iterating at a certain dataset.
  This dataset could be from \textsl{another} job's parameters, so we
  can for example iterate exactly over all new datasets not covered by
  a previous job.
\item[] \texttt{range}, which allows for iterating over a range of
  data.
\end{itemize}
The \texttt{range} options is based on the max/min values stored for
each column in the dataset.  Assuming that the chain is sorted, one
can for example set
\begin{python}
range={'ts_column': ('2016-01-01', '2016-01-31')}
\end{python}
in order to get rows within the specified range only.  Using
\texttt{range=} is quite costly, since it requires each row in the
dataset chain with dates within the range to be checked against the
range criterion.  Therefore, there is a \texttt{sloppy} version that
iterates over complete datasets in the chain that contains at least
one row with a date within the range.  This runs at full speed, and is
useful, for example, to very quickly produce histograms or plots of
subsets of a huge dataset.



\subsection{Job Execution Flow and Result Passing}

Execution of code in a method is either parallel or serial depending
on which function is used to encapsulate it.  There are three
functions in a method that are called from the Accelerator when a
method is running, and they are \texttt{prepare()},
\texttt{analysis()}, and \texttt{synthesis()}.  All three may exist in
the same method, and at least one is required.  When the method
executes, they are called one after the other.
\begin{itemize}
\item[] \texttt{prepare()} is executed first.  The returned value is
  available in the variable \texttt{prepare\_res}.
\item[] \texttt{analysis()} is run in parallel processes, one for each
  slice.  It is called after completion of, and actually forked from
  \texttt{prepare()}.  Common input parameters are \texttt{sliceno},
  holding the number of the current process instance, and
  \texttt{prepare\_res}.  The return value for each process becomes
  available in the \texttt{analysis\_res} variable.
\item[] \texttt{synthesis()} is called after the last
  \texttt{analysis()}-process is completed.  It is typically used to
  aggregate parallel results created by \texttt{analysis()} and takes
  both \texttt{prepare\_res} and \texttt{analysis\_res} as optional
  parameters.  The latter is an iterator of the results from the
  parallel processes.
\end{itemize}
Figure~\ref{fig:prepanasyn} shows the execution order from top to
bottom, and the data passed between functions in coloured branches.
\texttt{prepare()} is executed first, and its return value is
available to both the \texttt{analysis()} and \texttt{synthesis()}
functions.  There are \texttt{slices} (a configurable parameter)
number of parallel \texttt{analysis()} processes, and their output is
available to the \texttt{synthesis()} function, which is executed
last.

Return values from any of the three functions may be stored in the
job's directory making them available to other jobs.


\begin{figure}[t]
  \begin{center}
    \input{figures/prepanasyn.pdftex_t}
    \caption{Execution flow and result propagation in a method.}
    \label{fig:prepanasyn}
  \end{center}
\end{figure}


\begin{figure}[b]
  \begin{center}
    \input{figures/execflow.pdftex_t}
    \caption{Execution flow of a method.  The method takes optionally
      three kinds of parameters: \texttt{options}, \texttt{jobs},
      and \texttt{datasets}.}
    \label{fig:execflow}
  \end{center}
\end{figure}


\clearpage
\subsection{Job Parameters}
\label{sec:jobparams}
We've seen how completed jobs can be used as input to new
jobs.  Jobs are one of three kinds of input parameters that
a job can take.  Here the input parameters are summarised:
\begin{itemize}
\item[] \texttt{jobs}, a set, or list, of identifiers to previously executed jobs;
\item[] \texttt{options}, a dictionary of options; and
\item[] \texttt{datasets}, a set, or list, of input \textsl{datasets}.
\end{itemize}
See Figure~\ref{fig:execflow}.  Parameters are entered as global
variables early in the method's source.


\section{A Class Based Programming Model}
The Accelerator is based on an class based paradigm. Access to the
Accelerator's built in functions and parameters are typically done
through a few \textsl{objects} that are populated by the running
Accelerator.
