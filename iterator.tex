%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                                                          %
% Copyright (c) 2018 eBay Inc.                                             %
%                                                                          %
% Licensed under the Apache License, Version 2.0 (the "License");          %
% you may not use this file except in compliance with the License.         %
% You may obtain a copy of the License at                                  %
%                                                                          %
%  http://www.apache.org/licenses/LICENSE-2.0                              %
%                                                                          %
% Unless required by applicable law or agreed to in writing, software      %
% distributed under the License is distributed on an "AS IS" BASIS,        %
% WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. %
% See the License for the specific language governing permissions and      %
% limitations under the License.                                           %
%                                                                          %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Iterators are members of the \texttt{dataset} class.  They are
available in methods for streaming dataset data one row at a time into
a program.

The iterator iterates over one or more specified data columns.  Each
output is a tuple corresponding to the specified columns for one data
row.  In case of iterating over a single column, the output may
optionally be a scalar for more efficient computing.  Iterators can be
parallel, in \analysis, or sequential, in \prepare or \synthesis.
There are two iterators available,
\begin{itemize}
\item [] \texttt{iterate()}, for  single dataset iteration, and
\item [] \texttt{iterate\_chain()} for iterating over dataset chains.
\end{itemize}
Both iterators share these arguments


\starttable
  \RP \texttt{sliceno} & \textsl{mandatory} & Slice number to iterate
  over, or \mintinline{python}/None/ to iterate over all slices
  sequentially. \\

  \RP \texttt{columns} & \pyNone & Tuple of column labels or a single name if
  iterating over one column.\\

  \RP \texttt{hashlabel} & \pyNone & Name of hash column.  If the code relies
  on a dataset being hashed on a particular column, set this to make
  the iterator verify that it is the case.  Execution will terminate
  if the hashlabel is incorrect.\\

  \RP \texttt{rehash} & \pyFalse & Setting this to \mintinline{python}/True/
  will rehash the dataset on the fly based on the \texttt{hashlabel}
  column.  Ideally, rehashing should be made using the
  \texttt{dataset\_rehash} method~\ref{xx}.\\
  
  \RP \texttt{filters} & \pyNone & Filters decide which rows to include.
  Explained in section~\ref{xx}\\

  \RP \texttt{translators} & \pyNone & Translators transform data values.
  Explained in section~\ref{xx}\\
  
  \RP \texttt{status\_reporting} & \pyTrue & Give status when pressing
  \texttt{C-t}.  Unless manually \texttt{zip}ing iterators, this
  should be set to default \mintinline{python}/True/.  See
  \texttt{dataset.py} for full information.\\
\stoptable

\comment{heter det columns här men labels överallt annars?  Var heter det labels förutom hashlabel?}
In addition, \texttt{iterate\_chain} takes these arguments too
\starttable
  \RP \texttt{length}&$-1$& Number of datasets in a chain to iterate over.
  Default is $-1$, which corresponds to all datasets in a chain.\\
  
  \RP \texttt{range}& \pyNone& Filter rows based on a column's value being
  within a range.\\

  \RP \texttt{sloppy\_range}& \pyFalse & Used with \texttt{range}, but will
  iterate over full datasets for those datasets that have values
  within range.  This might be faster.\\
  
  \RP \texttt{reverse}& \pyFalse & Iterate chain
  backwards.  Default is to iterate forward, i.e.\ from oldest to
  newest dataset.\\

  \RP \texttt{stop\_ds}& \pyNone & The first dataset older than all datasets in the chain.\comment{duh}\\ 

  \RP \texttt{pre\_callback}& \pyNone & A function that will be called before
  iterating each dataset.\\

  \RP \texttt{post\_callback}& \pyNone & A function that will be called after
  iterating each dataset.\\
\stoptable


\section{Basic Iteration}
Basic use include iterating in parallel or serial over one dataset or
a chain of datasets.

\subsection*{Parallel Iterator Invocation}
For parallel iteration in \analysis, the iterator needs to know the
number of the current slice.  The following is an example of iteration
that happens independently in each slice.

\begin{python}
datasets = ('source',)

def analysis(sliceno):
    h = defaultdict(set)
    for user, item in datasets.source.iterate(sliceno, columns=('user', 'item',)):
        h[user].add(item)
\end{python}
The program creates dictionaries mapping \texttt{user}s to sets of
\texttt{item}s for the \texttt{source} dataset.  (If we assume that
the dataset is hashed~\ref{xx}, this operation is entirely parallel
and there is no need to merge all the results from the analysis
processes later.

\subsection*{Sequential Iterator Invocation}
By specifying the \texttt{sliceno} parameter to
\mintinline{python}/None/, the iterator will run through all slices of
the dataset, one at a time, like in this example
\begin{python}
def synthesis():
    h = defaultdict(set)
    for user, item in datasets.source.iterate(None, columns=('user', 'item',)):
        h[user].add(item)
\end{python}
Slices will be iterated one at at time in increasing order.

\subsection*{Iterate Over Chains}
To iterate over several datasets in a chain, use
\texttt{iterate\_chain}.  The following example will iterate over the
last three datasets in the chain.
\begin{python}
datasets = ('source',)

def analysis(sliceno):
    h = defaultdict(set)
    for user, item in datasets.source.iterate_chain(sliceno, columns=('user', 'item',), length=3):
        h[user].add(item)
\end{python}
using \texttt{iterate\_chain} without explicitly specifying
\texttt{length} will default to a \texttt{length} of $-1$, which
corresponds to all datasets in the chain.

Here is an interesting example that will iterate over the datasets in
the chain that are new since the last invocation of the method.
\begin{python}
datasets = ('source',)
jobids = ('previous',)

def analysis(sliceno):
    h = defaultdict(set)
    for user, item in datasets.source.iterate_chain(sliceno, columns=('user', 'item',), stop_id=jobids.params.source):
        h[user].add(item)
\end{python}
\comment{funkar detta?}  Assuming that \texttt{previous} is a jobid to
the previous invocation of the method, \texttt{jobids.params.source}
will be the input \texttt{source} dataset to that job.  



\subsection*{Special Cases, Iterating Over All or a Singe Column}
It is possible to iterate over all columns in a dataset by specifying
an empty list of column names, like this
\begin{python}
for items in dataset.source.iterate(sliceno, None):
    print(items)  # is a tuple of all columns
\end{python}
The iterator will output a tuples populated with all column values for
each row.  The columns will be in sorted column name order.

Iterators output tuples, but in the case of iterating over a single
column it is more efficient to output scalars instead.  Here are the
two different ways to iterate over a single column
\begin{python}
# alternative 1, use lists/tuples
for user in datasets.source.iterate(sliceno, ('USER',)):
    userset.add(user[0])  # user is a tuple

# alternative 2, specify column as string, not list
for user in datasets.source.iterate(sliceno, 'USER',):
    userset.add(user)
\end{python}
Both styles are supported by filters and translators introduced later
i this chapter.




\section{Halting Iteration}

Iteration over a dataset chain will continue until all data is
exhausted or some stop criteria is fulfilled.  There are several
mechanisms for stopping, and they may be combined in a single
expression.  If so, iteration will be over the shortest range of the
conditions.

\subsection*{Halting Using \texttt{length}}
\begin{python}
for user, item in datasets.source.iterate_chain(
    sliceno, ('user', 'item',),
    length = options.length):
\end{python}
This will iterate for the last \texttt{options.length} number of
datasets.  Note that a length of $-1$ is default and will iterate
without bounds.


\subsection*{Halting Using \texttt{stop\_id}}
Similar to using \texttt{length}, but will stop when reaching a
certain dataset.
\begin{python}
for user, item in datasets.source.iterate_chain(
    sliceno, ('user', 'item',),
    stop_jobid = datasets.stopjob):
\end{python}

A more advanced, but very useful, method is to stop at a dataset that
is input to another job
\subsection*{Halting Using Another Job's Input Parameters}
\begin{python}
for user, item in datasets.source.iterate_chain(
    sliceno, ('user', 'item',),
    stop_id = {jobids.previous: 'source',}):
\end{python}
This will iterate until reaching end or the dataset
\texttt{job\_params(jobids.preprocess).datasets.source}.



\section{Iterating Over a Data Range}
It is possible to iterate over rows having a specified column's value
within a certain range.  This works best on datasets that are sorted
on the specified column.  \comment{eller ja?}
\begin{python}
for user, item in datasets.source.iterate_chain(
    sliceno, ('user', 'item',),
    range={timestamp, ('2016-01-01', '2016-01-31'),}):
\end{python}
This example will limit the iterator to exactly the range of lines
that fulfill the range condition.  It is relatively costly to filter
each line, and there is a speed advantage by specifying
\texttt{sloppy\_range}, which will iterate over all datasets that
contain part of the range.



\section{Iterating in the Reverse Direction}
By default, iterating over a chain of dataset starts at the oldest
dataset and ends at the latest dataset.  This behavior can be
reversed by specifying \mintinline{python}/reverse=True/.  But note
that row iteration is always in the forward direction within each
dataset!
\begin{python}
for user, item in datasets.source.iterate_chain(
    sliceno, ('user', 'item',),
    reverse=True):
\end{python}



\section{Hashed Datasets and Hashing Datasets}
Hashing a dataset on a particular columns, see~\ref{xx} may simplify
the code in methods using the dataset.  This code will not work
properly if it turns out that it is being fed with unhashed datasets.
Therefore, it is a good idea to \emph{assert} the hashlabel by
entering it into the iterator function, like this
\begin{python}
s = {user: item for user, item datasets.source.iterate_chain( \
     sliceno, ('user', 'item',), hashlabel='user')}
\end{python}
Execution will terminate if the hashlabel is not correct.

It is possible to rehash the dataset on-the-fly.  This is done by
setting the \texttt{rehash} argument to the iterator to
\mintinline{python}/True/.  The preferred way to rehash is to use the
\texttt{dataset\_rehash} method~\ref{xx}, since it will store the
rehashed dataset for later use, which in most scenarios will be more
efficient.


\section{Translators}

Translators transform iterator output data values on-the-fly.  A
translator is either a callable or a \texttt{dict}.  Translators are
similar to \texttt{filters} (explained later), and always
executed \emph{before} filtering.  The idea behind translators and
filters is that they provide a way to modify code behavior by
supplying functions as iterator options.  Using translators and
filters, it is possible to write re-useable functions that can have
different behaviour depending on context.


\subsection*{Callable Translator}

A translator function is a function from an input \texttt{tuple} (of
column values) to an output \texttt{tuple} of the same length.
Individual items may be passed through or modified, and it is possible
to mix different columns with each other before sending them to the
iterator output.  Here is an example
\begin{python}
def merger(user, item):
    return ("%s:%s" % (user, item), None)
for merge, _ in datasets.source.iterate_chain(None, ('user', 'item',),
                                     translator=merger):
    ...
\end{python}
The purpose of this translator is to convert each
\mintinline{python}/(user, item)/ tuple to a string \texttt{user:item}.  This is
the first output of the translator and iterator and is stored in the
\texttt{merge} variable.  The second output variable is not used in
this application, but a variable still has to be assigned, so it is
set arbitrarily to \pyNone.



\subsection*{Translator \texttt{dict}}

One or more columns may be translated independently using a translator
dictionary.  Such a dictionary is specified
as \mintinline{python}/{name: translation}/.  A translation may be
either a \texttt{dict} or a callable.  Examples of both kinds are
shown below.  First an example illustrating the use of a
translation \texttt{dict}.  Here, integers are translated into more
comprehensible strings.
\begin{python}
mapper = {2: 'HUMANLIKE', 4: 'LABRADOR', 5: 'STARFISH',}
for animal in datasets.source.iterate(None, 'NUM_LEGS',
                                      translator={'NUM_LEGS': mapper,}):
    ...
\end{python}
\comment{note that is example is same as in overview!}
This translator will substitute the integers 2,4, and 5 into strings.
Items missing in a translation-\texttt{dict} yield \pyNone.  The next
example illustrates a callable inside a translator \texttt{dict}.  In
this example, each \texttt{user} string is output from the iterator
reading right-to-left.
\begin{python}
def reverse(x):
    return x[::-1]
for resu, item in dataset.source.iterate(None, ('USER', 'ITEM',),
                                         translator={'USER': reverse,}):
    ...
\end{python}
The \texttt{item} values will be passed through, but the \texttt{user}
strings will be reversed.



\section{Filters}
Filter are used to decide which output data rows that are allowed to
reach the iterator output.  Filters are run \emph{after} translators.
As for translators, filter are either a callables or a \texttt{dict}s.




\subsection*{Callable Filter}

Callable filters receive the iterator \texttt{tuple} as input.  The
output of a filter must be \pyTrue for the \texttt{tuple} to be output
from the iterator, otherwise the iterator skips and continues by
reading the next row.  The following two examples will iterate over
all animals that have at least two legs and a trunk.  The first
example is without filter, using an \texttt{if}-statement, and the
second example uses iterator \texttt{filters}.
\begin{python}
# Ex. 1. Using if-statement
for animal, nlegs, wtrunk in datasets.source.iterate(None,
    ('animal', num_legs', 'has_trunk'), filters=filter):
    if nlegs>=2 and wtrunk:
        ...

# Ex. 2.  Using iterator filters
filter = lambda line: line[1]>=2 and line[2]

for animal, nlegs, wtrunk in datasets.source.iterate(None,
    ('animal', num_legs', 'has_trunk'), filters=filter):
    ...
\end{python}
Note the indexing in the \texttt{lambda} function.  Index zero
corresponds to the \texttt{animal} column, which is not includes in
the filtering expression.



\subsection*{Filter Dict}

It is possible to filter on one or more columns independently using a
\texttt{dict}.  If there is more than one filter, all filters must be
\pyTrue for a line to be output from the iterator.
Below are two examples of filter \texttt{dict}s.  The first example
will remove all rows except the ones with valid users.
\begin{python}
# keep valid users only
validusers={'user1', 'user2', 'user3'}
filters={'user': validusers.__contains__}
\end{python}
The second example will only keep rows with valid users and movie
items.  The fact that book is \pyFalse is actually redundant, since
missing keys will never evaluate to \pyTrue and tgus result in
discarded lines.
\begin{python}
# keep valid users with movie items
validusers={'user1', 'user2', 'user3'}
validitems={'movie': True, 'book': False}
filters={'user': validusers.__contains__, 'item': validitems.get}
\end{python}




\subsection*{Filter by Column Values}


Column values could also be used directly, i.e.\ the values get
evaluated by Python.  For example, assume there is a column
\texttt{has\_trunk} with values being boolean integers, i.e.\ \texttt{1}
or \texttt{0}.  Animals with trunks may be iterated using
\begin{python}
for animal, wtrunk in datasets.source.iterate_chain(None, ('animal', 'has_trunk',),
                                       filters={'has_trunk': None}):
    trunked.add(animal)
\end{python}
This may seem strange at first, but it works because the key for
the \texttt{has\_trunk} column exists, and the value is
\pyNone, which is neither a callable nor a \texttt{dict}.




\section{Callback}
\label{sec:callback}
The iterator may be assigned callback functions that are called before
starting iterating a new dataset, and after the current dataset is
exhausted.  There are two independent callbacks for these two cases,
called \texttt{pre\_callback} and \texttt{post\_callback}.
If \mintinline{python}/sliceno=None/, i.e.\ iteration runs over all
slices of all datasets, it is even possible to have callback between
slice changes.

The example below will print the dataset identifier for each dataset
prior to iterating over it.
\begin{python}
# pre_callback once per dataset
def prefun(id):
    print(id)

for user, item in datasets.source.iterate(sliceno, ('user', 'item',),
                                       pre_callback=prefun):
    ...
\end{python}
Next is an example of an iterator running over all slices.  The
callback function is executed before each new slice is iterated.  Note
the difference between this example and the previous.  The callback
function in this example takes \emph{two} arguments, while the
previous takes only one.
\begin{python}
# callback once per slice
def prefun(jobid, sliceno):
    print(sliceno, jobid)

for user, item in datasets.source.iterate(None, ('user', 'item',),
                                       pre_callback=prefun):
    ...
\end{python}
The \texttt{post\_callback} function is defined similarly.



\subsection*{Skipping Datasets and Slices from Callbacks}
It is possible to skip dataset iterations by raising exceptions, as
follows.  To have the iterator skip a slice, do
\begin{python}
# Raise this in pre_callback to skip iterating the coming slice
# (if your callback doesn't want sliceno, this is the same as SkipJob)
raise SkipSlice
\end{python}
To skip the next dataset do
\begin{python}
# Raise this in pre_callback to skip iterating the coing job
raise SkipJob
\end{python}
And to abort iterating completely
\begin{python}
# Raise this to quit iterating, but with the side effect that
# post_callback will not run.
raise StopIteration
\end{python}
\comment{e denna definierad ens?}
